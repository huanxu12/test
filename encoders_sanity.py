#!/usr/bin/env python3
"""
Encoders Sanity Check for MineSLAM
æµ‹è¯•å¤šæ¨¡æ€ç¼–ç å™¨çš„è¾“å‡ºå½¢çŠ¶ã€FLOPså’Œæ˜¾å­˜ä½¿ç”¨
"""

import torch
import torch.nn as nn
import numpy as np
import time
from typing import Dict, Tuple, List
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.encoders import MultiModalEncoder

# å°è¯•å¯¼å…¥FLOPsè®¡ç®—å·¥å…·
try:
    from thop import profile, clever_format
    THOP_AVAILABLE = True
except ImportError:
    THOP_AVAILABLE = False
    print("Warning: thop not available, FLOPs calculation will be skipped")
    print("Install with: pip install thop")

# å°è¯•å¯¼å…¥torchinfo
try:
    from torchinfo import summary
    TORCHINFO_AVAILABLE = True
except ImportError:
    TORCHINFO_AVAILABLE = False
    print("Warning: torchinfo not available, detailed model info will be skipped")
    print("Install with: pip install torchinfo")


def create_sample_data(batch_size: int = 2, device: str = 'cpu') -> Dict[str, torch.Tensor]:
    """
    åˆ›å»ºæ ·æœ¬æ•°æ®ç”¨äºæµ‹è¯•
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        device: è®¾å¤‡
    
    Returns:
        åŒ…å«å„æ¨¡æ€æ ·æœ¬æ•°æ®çš„å­—å…¸
    """
    data_dict = {
        # RGBå›¾åƒ: [B, 3, 480, 640]
        'rgb': torch.randn(batch_size, 3, 480, 640, device=device),
        
        # æ·±åº¦å›¾åƒ: [B, 1, 480, 640]
        'depth': torch.randn(batch_size, 1, 480, 640, device=device),
        
        # çƒ­æˆåƒå›¾åƒ: [B, 1, 480, 640]
        'thermal': torch.randn(batch_size, 1, 480, 640, device=device),
        
        # LiDARç‚¹äº‘: [B, N, 4] (N=8192ä¸ªç‚¹)
        'lidar': torch.randn(batch_size, 8192, 4, device=device),
        
        # IMUåºåˆ—: [B, T, 6] (T=20ä¸ªæ—¶é—´æ­¥)
        'imu': torch.randn(batch_size, 20, 6, device=device),
    }
    
    return data_dict


def get_memory_usage() -> float:
    """è·å–GPUæ˜¾å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / (1024 ** 2)
    return 0.0


def calculate_flops(model: nn.Module, input_data: Dict[str, torch.Tensor]) -> Tuple[float, float]:
    """
    è®¡ç®—æ¨¡å‹FLOPså’Œå‚æ•°é‡
    
    Args:
        model: æ¨¡å‹
        input_data: è¾“å…¥æ•°æ®
    
    Returns:
        (FLOPs, å‚æ•°é‡)
    """
    if not THOP_AVAILABLE:
        return 0.0, 0.0
    
    try:
        # è®¡ç®—æ€»FLOPs
        total_flops = 0
        total_params = 0
        
        # åˆ†åˆ«è®¡ç®—å„ä¸ªç¼–ç å™¨çš„FLOPs
        for modality, data in input_data.items():
            if modality == 'rgb':
                flops, params = profile(model.rgb_encoder, inputs=(data,), verbose=False)
            elif modality == 'depth':
                flops, params = profile(model.depth_encoder, inputs=(data,), verbose=False)
            elif modality == 'thermal':
                flops, params = profile(model.thermal_encoder, inputs=(data,), verbose=False)
            elif modality == 'lidar':
                flops, params = profile(model.lidar_encoder, inputs=(data,), verbose=False)
            elif modality == 'imu':
                flops, params = profile(model.imu_encoder, inputs=(data,), verbose=False)
            else:
                continue
                
            total_flops += flops
            total_params += params
        
        return total_flops, total_params
        
    except Exception as e:
        print(f"FLOPs calculation failed: {e}")
        return 0.0, 0.0


def test_encoder_shapes(model: MultiModalEncoder, 
                       input_data: Dict[str, torch.Tensor],
                       device: str) -> Dict[str, torch.Tensor]:
    """
    æµ‹è¯•ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶
    
    Args:
        model: å¤šæ¨¡æ€ç¼–ç å™¨
        input_data: è¾“å…¥æ•°æ®
        device: è®¾å¤‡
    
    Returns:
        è¾“å‡ºtokenå­—å…¸
    """
    print("=" * 60)
    print("ENCODER SHAPE TESTING")
    print("=" * 60)
    
    # å°†æ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡
    model = model.to(device)
    model.eval()
    
    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
    input_data = {k: v.to(device) for k, v in input_data.items()}
    
    print("\nInput Shapes:")
    for modality, tensor in input_data.items():
        print(f"  {modality:8}: {list(tensor.shape)} ({tensor.dtype})")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        token_dict = model(input_data)
    
    print("\nOutput Token Shapes:")
    total_tokens = 0
    for modality, tokens in token_dict.items():
        num_tokens = tokens.shape[1]
        embedding_dim = tokens.shape[2]
        total_tokens += num_tokens
        print(f"  {modality:8}: {list(tokens.shape)} â†’ {num_tokens} tokens Ã— {embedding_dim}D")
    
    print(f"\nTotal Tokens: {total_tokens}")
    
    return token_dict


def test_memory_and_flops(model: MultiModalEncoder,
                         input_data: Dict[str, torch.Tensor],
                         device: str) -> Dict[str, float]:
    """
    æµ‹è¯•æ˜¾å­˜ä½¿ç”¨å’ŒFLOPs
    
    Args:
        model: å¤šæ¨¡æ€ç¼–ç å™¨
        input_data: è¾“å…¥æ•°æ®
        device: è®¾å¤‡
    
    Returns:
        æ€§èƒ½æŒ‡æ ‡å­—å…¸
    """
    print("\n" + "=" * 60)
    print("MEMORY AND FLOPS TESTING")
    print("=" * 60)
    
    model = model.to(device)
    input_data = {k: v.to(device) for k, v in input_data.items()}
    
    # æ¸…ç©ºæ˜¾å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    
    # æµ‹é‡æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nModel Parameters:")
    print(f"  Total: {total_params:,}")
    print(f"  Trainable: {trainable_params:,}")
    print(f"  Model Size: {total_params * 4 / (1024**2):.1f} MB (FP32)")
    
    # æµ‹é‡æ˜¾å­˜ä½¿ç”¨
    if torch.cuda.is_available():
        mem_before = torch.cuda.memory_allocated() / (1024 ** 2)
        
        with torch.no_grad():
            _ = model(input_data)
        
        mem_after = torch.cuda.memory_allocated() / (1024 ** 2)
        peak_mem = torch.cuda.max_memory_allocated() / (1024 ** 2)
        
        print(f"\nGPU Memory Usage:")
        print(f"  Before: {mem_before:.1f} MB")
        print(f"  After: {mem_after:.1f} MB") 
        print(f"  Peak: {peak_mem:.1f} MB")
        print(f"  Forward Pass: {mem_after - mem_before:.1f} MB")
    
    # è®¡ç®—FLOPs
    flops, _ = calculate_flops(model, input_data)
    if flops > 0:
        flops_str = clever_format([flops], "%.3f")[0] if THOP_AVAILABLE else f"{flops:.2e}"
        print(f"\nComputational Complexity:")
        print(f"  FLOPs: {flops_str}")
        print(f"  FLOPs per token: {flops / model.get_total_tokens(list(input_data.keys())):.2e}")
    
    return {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'memory_usage_mb': mem_after - mem_before if torch.cuda.is_available() else 0,
        'flops': flops
    }


def test_amp_support(model: MultiModalEncoder,
                    input_data: Dict[str, torch.Tensor],
                    device: str) -> bool:
    """
    æµ‹è¯•è‡ªåŠ¨æ··åˆç²¾åº¦(AMP)æ”¯æŒ
    
    Args:
        model: å¤šæ¨¡æ€ç¼–ç å™¨
        input_data: è¾“å…¥æ•°æ®
        device: è®¾å¤‡
    
    Returns:
        æ˜¯å¦æ”¯æŒAMP
    """
    print("\n" + "=" * 60)
    print("AUTOMATIC MIXED PRECISION (AMP) TESTING")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("CUDA not available, skipping AMP test")
        return False
    
    model = model.to(device)
    input_data = {k: v.to(device) for k, v in input_data.items()}
    
    try:
        # æµ‹è¯•AMPå‰å‘ä¼ æ’­
        with torch.cuda.amp.autocast():
            with torch.no_grad():
                token_dict = model(input_data)
        
        print("âœ“ AMP Forward Pass: SUCCESS")
        
        # æ£€æŸ¥è¾“å‡ºæ•°æ®ç±»å‹
        for modality, tokens in token_dict.items():
            print(f"  {modality:8}: {tokens.dtype}")
        
        # æµ‹è¯•æ¢¯åº¦ç¼©æ”¾ï¼ˆéœ€è¦åˆ›å»ºæŸå¤±å‡½æ•°ï¼‰
        model.train()
        scaler = torch.cuda.amp.GradScaler()
        
        with torch.cuda.amp.autocast():
            token_dict = model(input_data)
            # åˆ›å»ºè™šæ‹ŸæŸå¤±
            loss = sum(tokens.mean() for tokens in token_dict.values())
        
        scaler.scale(loss).backward()
        scaler.step(torch.optim.Adam(model.parameters()))
        scaler.update()
        
        print("âœ“ AMP Backward Pass: SUCCESS")
        print("âœ“ Gradient Scaling: SUCCESS")
        
        return True
        
    except Exception as e:
        print(f"âœ— AMP Test Failed: {e}")
        return False


def benchmark_inference_speed(model: MultiModalEncoder,
                             input_data: Dict[str, torch.Tensor],
                             device: str,
                             num_runs: int = 100) -> Dict[str, float]:
    """
    åŸºå‡†æ¨ç†é€Ÿåº¦æµ‹è¯•
    
    Args:
        model: å¤šæ¨¡æ€ç¼–ç å™¨
        input_data: è¾“å…¥æ•°æ®
        device: è®¾å¤‡  
        num_runs: è¿è¡Œæ¬¡æ•°
    
    Returns:
        é€Ÿåº¦æŒ‡æ ‡å­—å…¸
    """
    print("\n" + "=" * 60)
    print("INFERENCE SPEED BENCHMARK")
    print("=" * 60)
    
    model = model.to(device)
    model.eval()
    input_data = {k: v.to(device) for k, v in input_data.items()}
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(10):
            _ = model(input_data)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # åŸºå‡†æµ‹è¯•
    times = []
    for _ in range(num_runs):
        start_time = time.time()
        
        with torch.no_grad():
            _ = model(input_data)
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        end_time = time.time()
        times.append(end_time - start_time)
    
    times = np.array(times[10:])  # å»æ‰å‰10æ¬¡ï¼ˆé¢å¤–é¢„çƒ­ï¼‰
    
    avg_time = np.mean(times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    std_time = np.std(times) * 1000
    min_time = np.min(times) * 1000
    max_time = np.max(times) * 1000
    
    batch_size = input_data['rgb'].shape[0]
    fps = batch_size / (avg_time / 1000)
    
    print(f"\nInference Speed (batch_size={batch_size}, runs={len(times)}):")
    print(f"  Average: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"  Min: {min_time:.2f} ms")
    print(f"  Max: {max_time:.2f} ms")
    print(f"  FPS: {fps:.1f}")
    
    return {
        'avg_time_ms': avg_time,
        'std_time_ms': std_time,
        'min_time_ms': min_time,
        'max_time_ms': max_time,
        'fps': fps
    }


def print_model_summary(model: MultiModalEncoder):
    """æ‰“å°æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
    if not TORCHINFO_AVAILABLE:
        return
    
    print("\n" + "=" * 60)
    print("MODEL ARCHITECTURE SUMMARY")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ ·æœ¬è¾“å…¥ç”¨äºsummary
        sample_rgb = torch.randn(1, 3, 480, 640)
        
        print("\nRGB Encoder:")
        summary(model.rgb_encoder, input_data=sample_rgb, verbose=0)
        
        print("\nLiDAR Encoder:")
        sample_lidar = torch.randn(1, 1024, 4)
        summary(model.lidar_encoder, input_data=sample_lidar, verbose=0)
        
        print("\nIMU Encoder:")
        sample_imu = torch.randn(1, 20, 6)
        summary(model.imu_encoder, input_data=sample_imu, verbose=0)
        
    except Exception as e:
        print(f"Model summary failed: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("MineSLAM Multi-Modal Encoders Sanity Check")
    print("=" * 80)
    
    # æ£€æŸ¥è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Device: {device}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
    
    # é…ç½®å‚æ•°
    embedding_dim = 512
    voxel_size = 0.05
    batch_size = 2
    
    print(f"\nConfiguration:")
    print(f"  Embedding Dim: {embedding_dim}")
    print(f"  Input Resolution: 480Ã—640")
    print(f"  IMU Window: T=20")
    print(f"  LiDAR Voxel Size: {voxel_size}m")
    print(f"  Batch Size: {batch_size}")
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nInitializing MultiModalEncoder...")
    model = MultiModalEncoder(embedding_dim=embedding_dim, voxel_size=voxel_size)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print(f"\nCreating sample data...")
    input_data = create_sample_data(batch_size=batch_size, device=device)
    
    # æ‰“å°æ¨¡å‹æ¶æ„æ‘˜è¦
    print_model_summary(model)
    
    # æµ‹è¯•è¾“å‡ºå½¢çŠ¶
    token_dict = test_encoder_shapes(model, input_data, device)
    
    # æµ‹è¯•å†…å­˜å’ŒFLOPs
    performance_metrics = test_memory_and_flops(model, input_data, device)
    
    # æµ‹è¯•AMPæ”¯æŒ
    amp_supported = test_amp_support(model, input_data, device)
    
    # åŸºå‡†æ¨ç†é€Ÿåº¦
    speed_metrics = benchmark_inference_speed(model, input_data, device)
    
    # æ€»ç»“æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("FINAL SUMMARY")
    print("=" * 80)
    
    print(f"\nâœ“ Multi-Modal Encoder Successfully Initialized")
    print(f"âœ“ All Modality Encoders Working Correctly")
    print(f"âœ“ Unified Token Output: {embedding_dim}D")
    print(f"âœ“ Total Parameters: {performance_metrics['total_params']:,}")
    print(f"âœ“ Model Size: {performance_metrics['total_params'] * 4 / (1024**2):.1f} MB")
    
    if torch.cuda.is_available():
        print(f"âœ“ GPU Memory Usage: {performance_metrics['memory_usage_mb']:.1f} MB")
        print(f"âœ“ Inference Speed: {speed_metrics['avg_time_ms']:.1f} ms ({speed_metrics['fps']:.1f} FPS)")
    
    print(f"âœ“ AMP Support: {'Yes' if amp_supported else 'No'}")
    
    # å„æ¨¡æ€tokenç»Ÿè®¡
    print(f"\nToken Statistics:")
    for modality, tokens in token_dict.items():
        num_tokens = tokens.shape[1]
        print(f"  {modality:8}: {num_tokens:3d} tokens")
    
    total_tokens = sum(tokens.shape[1] for tokens in token_dict.values())
    print(f"  {'Total':8}: {total_tokens:3d} tokens")
    
    print(f"\nğŸ‰ Encoders sanity check completed successfully!")


if __name__ == '__main__':
    main()