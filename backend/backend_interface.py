"""
MineSLAM Backend Optimization Main Interface
MineSLAMåç«¯ä¼˜åŒ–ä¸»æ¥å£ï¼šbackend_optimize()å‡½æ•°ï¼Œé›†æˆå®Œæ•´æµæ°´çº¿
"""

import numpy as np
import torch
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
import warnings
import json

from .graph_optimizer import PoseGraphOptimizer, PoseVertex
from .loop_detector import LoopDetector, KeyFrame
from .trajectory_evaluator import TrajectoryEvaluator, TrajectoryVisualizer
from .semantic_factors import SemanticFactor, SemanticObservation, LandmarkVertex


def backend_optimize(trajectory_data: Dict[str, Any],
                    loop_detection_params: Optional[Dict] = None,
                    optimization_params: Optional[Dict] = None,
                    output_dir: str = "outputs/backend") -> Dict[str, Any]:
    """
    MineSLAMåç«¯ä¼˜åŒ–ä¸»å‡½æ•°

    Args:
        trajectory_data: è½¨è¿¹æ•°æ®å­—å…¸ï¼ŒåŒ…å«ï¼š
            - poses: List[Dict] ä½å§¿åºåˆ—
            - keyframes: List[Dict] å…³é”®å¸§æ•°æ®
            - semantic_observations: List[Dict] è¯­ä¹‰è§‚æµ‹
            - ground_truth: Optional[np.ndarray] çœŸå€¼è½¨è¿¹
        loop_detection_params: å›ç¯æ£€æµ‹å‚æ•°
        optimization_params: ä¼˜åŒ–å‚æ•°
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        DictåŒ…å«ä¼˜åŒ–ç»“æœã€è¯„ä¼°æŒ‡æ ‡ã€å¯è§†åŒ–è·¯å¾„
    """

    # è®¾ç½®é»˜è®¤å‚æ•°
    if loop_detection_params is None:
        loop_detection_params = {
            'descriptor_type': 'simple',
            'similarity_threshold': 0.8,
            'temporal_consistency': 50
        }

    if optimization_params is None:
        optimization_params = {
            'max_iterations': 30,
            'convergence_threshold': 1e-6,
            'use_robust_kernel': True
        }

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    print(f"ğŸš€ Starting MineSLAM Backend Optimization...")
    print(f"Output directory: {output_path}")

    # æ­¥éª¤1: æ„å»ºä½å§¿å›¾
    print("ğŸ“Š Building pose graph...")
    optimizer = PoseGraphOptimizer()

    # è§£æä½å§¿æ•°æ®
    poses = _parse_pose_data(trajectory_data['poses'])
    optimizer.add_pose_sequence(poses)

    print(f"  âœ… Added {len(poses)} poses to graph")

    # è®°å½•ä¼˜åŒ–å‰è½¨è¿¹
    trajectory_before = optimizer.get_trajectory()

    # æ­¥éª¤2: å›ç¯æ£€æµ‹
    print("ğŸ” Performing loop detection...")
    loop_detector = LoopDetector(**loop_detection_params)

    # æ·»åŠ å…³é”®å¸§
    keyframes = _parse_keyframe_data(trajectory_data['keyframes'])
    valid_keyframes = 0

    for keyframe in keyframes:
        if loop_detector.add_keyframe(keyframe):
            valid_keyframes += 1

    print(f"  âœ… Added {valid_keyframes}/{len(keyframes)} valid keyframes")

    # æ£€æµ‹å›ç¯
    loop_closures = _detect_loop_closures(loop_detector, keyframes, poses)
    print(f"  ğŸ”— Detected {len(loop_closures)} loop closures")

    # æ·»åŠ å›ç¯çº¦æŸ
    for loop_closure in loop_closures:
        optimizer.add_loop_closure(
            from_id=loop_closure['from_id'],
            to_id=loop_closure['to_id'],
            relative_pose=loop_closure['relative_pose'],
            confidence=loop_closure['confidence']
        )

    # æ­¥éª¤3: è¯­ä¹‰çº¦æŸ
    if 'semantic_observations' in trajectory_data:
        print("ğŸ¯ Adding semantic constraints...")
        semantic_observations = _parse_semantic_data(trajectory_data['semantic_observations'])
        optimizer.add_semantic_constraints(semantic_observations)
        print(f"  âœ… Added {len(semantic_observations)} semantic constraints")

    # æ­¥éª¤4: å›¾ä¼˜åŒ–
    print("âš™ï¸ Performing graph optimization...")
    optimization_result = optimizer.backend_optimize(
        max_iterations=optimization_params['max_iterations']
    )

    print(f"  âœ… Optimization converged: {optimization_result['converged']}")
    print(f"  ğŸ“‰ Chi2 reduction: {optimization_result['chi2_reduction']:.3f}")

    # è®°å½•ä¼˜åŒ–åè½¨è¿¹
    trajectory_after = optimizer.get_trajectory()

    # æ­¥éª¤5: è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
    print("ğŸ“ˆ Evaluating optimization results...")
    evaluator = TrajectoryEvaluator()

    # å¦‚æœæœ‰çœŸå€¼ï¼Œè®¡ç®—ATEæ”¹è¿›
    evaluation_results = {}
    if 'ground_truth' in trajectory_data and trajectory_data['ground_truth'] is not None:
        gt_trajectory = trajectory_data['ground_truth']
        evaluator.set_ground_truth(gt_trajectory)

        improvement = evaluator.evaluate_optimization_improvement(
            trajectory_before, trajectory_after
        )
        evaluation_results = improvement

        print(f"  ğŸ“Š ATE improvement: {improvement['ate_improvement_percentage']:.1f}%")
        print(f"  ğŸ¯ Target achieved (â‰¥10%): {improvement['improvement_achieved']}")
    else:
        print("  âš ï¸  No ground truth available for ATE evaluation")

    # æ­¥éª¤6: å¯è§†åŒ–å’Œä¿å­˜ç»“æœ
    print("ğŸ¨ Generating visualizations...")
    visualizer = TrajectoryVisualizer(str(output_path))

    visualization_paths = {}

    # è½¨è¿¹å¯¹æ¯”å›¾
    if 'ground_truth' in trajectory_data and trajectory_data['ground_truth'] is not None:
        comparison_path = visualizer.plot_trajectory_comparison(
            trajectory_data['ground_truth'],
            trajectory_before,
            trajectory_after,
            save_path=output_path / "trajectory_comparison.png"
        )
        visualization_paths['trajectory_comparison'] = comparison_path

    # 3Dåœ°æ ‡å›¾
    optimized_landmarks = optimization_result.get('optimized_landmarks', {})
    if optimized_landmarks:
        landmarks_path = visualizer.plot_landmarks_3d(
            optimized_landmarks,
            trajectory_after,
            save_path=output_path / "landmarks_3d.png"
        )
        visualization_paths['landmarks_3d'] = landmarks_path

    # ä½å§¿ä¸ç¡®å®šæ€§
    pose_uncertainties = optimizer.get_pose_uncertainties()
    if pose_uncertainties:
        uncertainties_path = visualizer.plot_pose_uncertainties(
            trajectory_after,
            pose_uncertainties,
            save_path=output_path / "pose_uncertainties.png"
        )
        visualization_paths['pose_uncertainties'] = uncertainties_path

    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    report_path = visualizer.save_evaluation_report(
        evaluation_results,
        optimization_result,
        save_path=output_path / "optimization_report.json"
    )
    visualization_paths['evaluation_report'] = report_path

    print(f"  âœ… Visualizations saved to: {output_path}")

    # è¿”å›å®Œæ•´ç»“æœ
    results = {
        'optimization_result': optimization_result,
        'evaluation_results': evaluation_results,
        'trajectory_before': trajectory_before,
        'trajectory_after': trajectory_after,
        'loop_closures': loop_closures,
        'visualization_paths': visualization_paths,
        'output_directory': str(output_path),
        'summary': {
            'num_poses': len(poses),
            'num_loop_closures': len(loop_closures),
            'optimization_converged': optimization_result['converged'],
            'chi2_reduction': optimization_result['chi2_reduction'],
            'ate_improvement_percentage': evaluation_results.get('ate_improvement_percentage', 0.0),
            'target_achieved': evaluation_results.get('improvement_achieved', False)
        }
    }

    print("âœ… Backend optimization completed successfully!")
    return results


def _parse_pose_data(poses_data: List[Dict]) -> List[PoseVertex]:
    """è§£æä½å§¿æ•°æ®"""
    poses = []

    for i, pose_dict in enumerate(poses_data):
        try:
            # å…¼å®¹ä¸åŒæ ¼å¼
            if 'position' in pose_dict and 'orientation' in pose_dict:
                position = np.array(pose_dict['position'])
                orientation = np.array(pose_dict['orientation'])
            elif 'pose' in pose_dict:
                pose_array = np.array(pose_dict['pose'])
                position = pose_array[:3]
                orientation = pose_array[3:6] if len(pose_array) >= 6 else np.zeros(3)
            else:
                # å‡è®¾ç›´æ¥æ˜¯6DoFæ•°ç»„
                pose_array = np.array(pose_dict)
                position = pose_array[:3]
                orientation = pose_array[3:6] if len(pose_array) >= 6 else np.zeros(3)

            pose = PoseVertex(
                id=i,
                timestamp=pose_dict.get('timestamp', i * 0.1),
                position=position,
                orientation=orientation
            )
            poses.append(pose)

        except Exception as e:
            warnings.warn(f"Failed to parse pose {i}: {e}")

    return poses


def _parse_keyframe_data(keyframes_data: List[Dict]) -> List[KeyFrame]:
    """è§£æå…³é”®å¸§æ•°æ®"""
    keyframes = []

    for i, kf_dict in enumerate(keyframes_data):
        try:
            # åˆ›å»ºå…³é”®å¸§
            keyframe = KeyFrame(
                frame_id=kf_dict.get('frame_id', i),
                timestamp=kf_dict.get('timestamp', i * 0.1),
                rgb_image=kf_dict.get('rgb_image'),
                depth_image=kf_dict.get('depth_image'),
                thermal_image=kf_dict.get('thermal_image'),
                lidar_points=kf_dict.get('lidar_points'),
                pose=kf_dict.get('pose'),
                data_source=kf_dict.get('data_source', 'real_sensors')
            )
            keyframes.append(keyframe)

        except Exception as e:
            warnings.warn(f"Failed to parse keyframe {i}: {e}")

    return keyframes


def _detect_loop_closures(loop_detector: LoopDetector,
                         keyframes: List[KeyFrame],
                         poses: List[PoseVertex]) -> List[Dict]:
    """æ£€æµ‹å›ç¯é—­åˆ"""
    loop_closures = []

    # å¯¹æ¯ä¸ªå…³é”®å¸§æ£€æµ‹å›ç¯
    for i, keyframe in enumerate(keyframes):
        if i < 50:  # è·³è¿‡å‰50å¸§ä»¥ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§
            continue

        try:
            candidates = loop_detector.detect_loop_closure(keyframe, k_candidates=3)

            for candidate in candidates:
                # éªŒè¯å›ç¯è´¨é‡
                if candidate.similarity_score > 0.8 and candidate.geometric_verification:

                    # è®¡ç®—ç›¸å¯¹ä½å§¿ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                    from_pose = poses[candidate.match_id].to_se3()
                    to_pose = poses[candidate.query_id].to_se3()
                    relative_pose = from_pose.inverse() * to_pose

                    loop_closure = {
                        'from_id': candidate.match_id,
                        'to_id': candidate.query_id,
                        'relative_pose': relative_pose,
                        'confidence': candidate.similarity_score,
                        'similarity_score': candidate.similarity_score
                    }

                    loop_closures.append(loop_closure)

        except Exception as e:
            warnings.warn(f"Loop detection failed for keyframe {i}: {e}")

    return loop_closures


def _parse_semantic_data(semantic_data: List[Dict]) -> List[Dict]:
    """è§£æè¯­ä¹‰è§‚æµ‹æ•°æ®"""
    observations = []

    for obs_dict in semantic_data:
        try:
            observation = {
                'pose_id': obs_dict['pose_id'],
                'world_position': np.array(obs_dict['world_position']),
                'image_observation': np.array(obs_dict['image_observation']),
                'class': obs_dict['semantic_class'],
                'confidence': obs_dict['confidence'],
                'timestamp': obs_dict.get('timestamp', 0.0)
            }
            observations.append(observation)

        except Exception as e:
            warnings.warn(f"Failed to parse semantic observation: {e}")

    return observations