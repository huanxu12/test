"""
Kendall Uncertainty Loss Weighting for Multi-Task Learning
åŸºäºKendallä¸ç¡®å®šæ€§çš„å¤šä»»åŠ¡æŸå¤±æƒé‡å­¦ä¹ 
å‚è€ƒ: "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics"
"""

import torch
import torch.nn as nn
import math
from typing import Dict, List, Optional


class KendallUncertainty(nn.Module):
    """
    Kendallä¸ç¡®å®šæ€§æŸå¤±æƒé‡å­¦ä¹ å™¨
    é€šè¿‡å­¦ä¹ ä»»åŠ¡ç›¸å…³çš„ä¸ç¡®å®šæ€§å‚æ•°Ïƒæ¥åŠ¨æ€å¹³è¡¡å¤šä»»åŠ¡æŸå¤±
    """

    def __init__(self, num_tasks: int = 3,
                 init_log_var: float = 0.0,
                 min_log_var: float = -10.0,
                 max_log_var: float = 5.0):
        """
        Args:
            num_tasks: ä»»åŠ¡æ•°é‡ï¼ˆpose, detection, gateï¼‰
            init_log_var: log(ÏƒÂ²)çš„åˆå§‹å€¼
            min_log_var: log(ÏƒÂ²)çš„æœ€å°å€¼ï¼ˆé¿å…æƒé‡è¿‡å¤§ï¼‰
            max_log_var: log(ÏƒÂ²)çš„æœ€å¤§å€¼ï¼ˆé¿å…æƒé‡è¿‡å°ï¼‰
        """
        super().__init__()

        self.num_tasks = num_tasks
        self.min_log_var = min_log_var
        self.max_log_var = max_log_var

        # å¯å­¦ä¹ çš„log(ÏƒÂ²)å‚æ•°
        self.log_vars = nn.Parameter(
            torch.full((num_tasks,), init_log_var, dtype=torch.float32)
        )

        # ä»»åŠ¡åç§°æ˜ å°„
        self.task_names = ['pose', 'detection', 'gate']

        print(f"KendallUncertainty initialized: {num_tasks} tasks, "
              f"init_log_var={init_log_var}, range=[{min_log_var}, {max_log_var}]")

    def forward(self, losses: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å¸¦ä¸ç¡®å®šæ€§æƒé‡çš„æ€»æŸå¤±

        Args:
            losses: å„ä»»åŠ¡æŸå¤±å­—å…¸
                - 'pose': å§¿æ€ä¼°è®¡æŸå¤±
                - 'detection': æ£€æµ‹æŸå¤±
                - 'gate': é—¨æ§æŸå¤±

        Returns:
            weighted_losses: åŠ æƒæŸå¤±å­—å…¸
        """
        # é™åˆ¶log_varsåœ¨åˆç†èŒƒå›´å†…
        clamped_log_vars = torch.clamp(self.log_vars, self.min_log_var, self.max_log_var)

        # è®¡ç®—ä¸ç¡®å®šæ€§æƒé‡: w_i = 1/(2ÏƒÂ²)
        uncertainties = torch.exp(clamped_log_vars)  # ÏƒÂ²
        weights = 1.0 / (2.0 * uncertainties)       # 1/(2ÏƒÂ²)

        weighted_losses = {}
        total_loss = 0.0

        for i, task_name in enumerate(self.task_names):
            if task_name in losses:
                # ä¸»è¦æŸå¤±é¡¹: w_i * L_i
                task_loss = losses[task_name]
                weighted_task_loss = weights[i] * task_loss

                # æ­£åˆ™åŒ–é¡¹: log(Ïƒ)ï¼ˆé˜²æ­¢Ïƒè¶‹å‘0ï¼‰
                regularization = 0.5 * clamped_log_vars[i]

                # æ€»çš„åŠ æƒæŸå¤±
                final_task_loss = weighted_task_loss + regularization

                weighted_losses[f'weighted_{task_name}'] = final_task_loss
                weighted_losses[f'{task_name}_weight'] = weights[i]
                weighted_losses[f'{task_name}_sigma'] = torch.sqrt(uncertainties[i])

                total_loss += final_task_loss

        weighted_losses['total_loss'] = total_loss

        return weighted_losses

    def get_weights(self) -> Dict[str, float]:
        """è·å–å½“å‰æƒé‡å€¼ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰"""
        with torch.no_grad():
            clamped_log_vars = torch.clamp(self.log_vars, self.min_log_var, self.max_log_var)
            uncertainties = torch.exp(clamped_log_vars)
            weights = 1.0 / (2.0 * uncertainties)

            weight_dict = {}
            for i, task_name in enumerate(self.task_names):
                weight_dict[f'{task_name}_weight'] = weights[i].item()
                weight_dict[f'{task_name}_sigma'] = torch.sqrt(uncertainties[i]).item()
                weight_dict[f'{task_name}_log_var'] = clamped_log_vars[i].item()

            return weight_dict

    def reset_parameters(self, init_log_var: float = 0.0):
        """é‡ç½®å‚æ•°"""
        with torch.no_grad():
            self.log_vars.fill_(init_log_var)


class AdaptiveKendallUncertainty(KendallUncertainty):
    """
    è‡ªé€‚åº”Kendallä¸ç¡®å®šæ€§
    æ ¹æ®è®­ç»ƒè¿‡ç¨‹åŠ¨æ€è°ƒæ•´ä¸ç¡®å®šæ€§å‚æ•°çš„å­¦ä¹ ç‡
    """

    def __init__(self, num_tasks: int = 3,
                 init_log_var: float = 0.0,
                 min_log_var: float = -10.0,
                 max_log_var: float = 5.0,
                 adaptation_rate: float = 0.01,
                 target_balance: float = 0.33):
        """
        Args:
            adaptation_rate: è‡ªé€‚åº”å­¦ä¹ ç‡
            target_balance: ç›®æ ‡å¹³è¡¡æ¯”ä¾‹ï¼ˆç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä»»åŠ¡æŸå¤±å æ€»æŸå¤±çš„1/3ï¼‰
        """
        super().__init__(num_tasks, init_log_var, min_log_var, max_log_var)

        self.adaptation_rate = adaptation_rate
        self.target_balance = target_balance

        # è®°å½•æŸå¤±å†å²ç”¨äºè‡ªé€‚åº”è°ƒæ•´
        self.loss_history = {name: [] for name in self.task_names}
        self.history_length = 100

        print(f"AdaptiveKendallUncertainty initialized: "
              f"adaptation_rate={adaptation_rate}, target_balance={target_balance}")

    def forward(self, losses: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è‡ªé€‚åº”è®¡ç®—å¸¦ä¸ç¡®å®šæ€§æƒé‡çš„æ€»æŸå¤±
        """
        # æ›´æ–°æŸå¤±å†å²
        with torch.no_grad():
            for task_name in self.task_names:
                if task_name in losses:
                    self.loss_history[task_name].append(losses[task_name].item())
                    if len(self.loss_history[task_name]) > self.history_length:
                        self.loss_history[task_name].pop(0)

        # è‡ªé€‚åº”è°ƒæ•´log_vars
        self._adaptive_adjustment()

        # è®¡ç®—åŠ æƒæŸå¤±
        return super().forward(losses)

    def _adaptive_adjustment(self):
        """åŸºäºæŸå¤±å†å²è‡ªé€‚åº”è°ƒæ•´log_vars"""
        if not self.training:
            return

        with torch.no_grad():
            # è®¡ç®—å„ä»»åŠ¡çš„ç›¸å¯¹æŸå¤±æ¯”ä¾‹
            recent_losses = {}
            total_recent_loss = 0.0

            for task_name in self.task_names:
                if len(self.loss_history[task_name]) >= 10:  # è‡³å°‘æœ‰10ä¸ªæ ·æœ¬
                    recent_avg = sum(self.loss_history[task_name][-10:]) / 10
                    recent_losses[task_name] = recent_avg
                    total_recent_loss += recent_avg

            if total_recent_loss > 0:
                for i, task_name in enumerate(self.task_names):
                    if task_name in recent_losses:
                        current_ratio = recent_losses[task_name] / total_recent_loss

                        # å¦‚æœæŸä»»åŠ¡æŸå¤±æ¯”ä¾‹è¿‡é«˜ï¼Œå¢åŠ å…¶log_varï¼ˆé™ä½æƒé‡ï¼‰
                        if current_ratio > self.target_balance * 1.5:
                            self.log_vars[i] += self.adaptation_rate
                        # å¦‚æœæŸä»»åŠ¡æŸå¤±æ¯”ä¾‹è¿‡ä½ï¼Œé™ä½å…¶log_varï¼ˆå¢åŠ æƒé‡ï¼‰
                        elif current_ratio < self.target_balance * 0.5:
                            self.log_vars[i] -= self.adaptation_rate

                        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
                        self.log_vars[i] = torch.clamp(
                            self.log_vars[i], self.min_log_var, self.max_log_var
                        )


# å·¥å‚å‡½æ•°
def create_kendall_uncertainty(uncertainty_type: str = 'basic',
                              num_tasks: int = 3,
                              **kwargs) -> KendallUncertainty:
    """
    åˆ›å»ºKendallä¸ç¡®å®šæ€§æ¨¡å—

    Args:
        uncertainty_type: 'basic' æˆ– 'adaptive'
        num_tasks: ä»»åŠ¡æ•°é‡
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        uncertainty_module: Kendallä¸ç¡®å®šæ€§æ¨¡å—
    """
    if uncertainty_type == 'adaptive':
        return AdaptiveKendallUncertainty(num_tasks=num_tasks, **kwargs)
    else:
        return KendallUncertainty(num_tasks=num_tasks, **kwargs)


# æ¨¡å—æµ‹è¯•
if __name__ == '__main__':
    print("ğŸ§ª Testing Kendall Uncertainty...")

    # åˆ›å»ºåŸºç¡€Kendallä¸ç¡®å®šæ€§
    kendall = KendallUncertainty(num_tasks=3, init_log_var=0.0)

    # æ¨¡æ‹ŸæŸå¤±
    losses = {
        'pose': torch.tensor(0.1),
        'detection': torch.tensor(0.5),
        'gate': torch.tensor(0.02)
    }

    print(f"Input losses: {losses}")

    # è®¡ç®—åŠ æƒæŸå¤±
    weighted_losses = kendall(losses)

    print(f"\nWeighted losses:")
    for key, value in weighted_losses.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.item():.6f}")
        else:
            print(f"  {key}: {value}")

    # è·å–æƒé‡ä¿¡æ¯
    weights = kendall.get_weights()
    print(f"\nCurrent weights:")
    for key, value in weights.items():
        print(f"  {key}: {value:.6f}")

    # æµ‹è¯•è‡ªé€‚åº”ç‰ˆæœ¬
    print(f"\nğŸ”§ Testing Adaptive Kendall Uncertainty...")
    adaptive_kendall = AdaptiveKendallUncertainty(
        num_tasks=3,
        adaptation_rate=0.01,
        target_balance=0.33
    )

    adaptive_kendall.train()

    # æ¨¡æ‹Ÿå¤šè½®è®­ç»ƒ
    for step in range(20):
        # æ¨¡æ‹Ÿä¸å¹³è¡¡çš„æŸå¤±
        losses = {
            'pose': torch.tensor(0.1 * (1 + 0.1 * step)),
            'detection': torch.tensor(0.5 * (1 - 0.05 * step)),
            'gate': torch.tensor(0.02)
        }

        weighted_losses = adaptive_kendall(losses)

        if step % 5 == 0:
            print(f"\nStep {step}:")
            print(f"  Total loss: {weighted_losses['total_loss'].item():.6f}")
            weights = adaptive_kendall.get_weights()
            for task in ['pose', 'detection', 'gate']:
                w = weights[f'{task}_weight']
                s = weights[f'{task}_sigma']
                print(f"  {task}: weight={w:.4f}, sigma={s:.4f}")

    print("\nâœ… Kendall Uncertainty test completed!")