"""
Fixed Kendall Uncertainty Loss Weighting for Multi-Task Learning
ä¿®å¤ç‰ˆKendallä¸ç¡®å®šæ€§å¤šä»»åŠ¡æŸå¤±æƒé‡å­¦ä¹  - è§£å†³ä¸¥é‡æƒé‡å¤±è¡¡é—®é¢˜

åŸé—®é¢˜: ä½å§¿æƒé‡11013 vs æ£€æµ‹æƒé‡0.003 (æ¯”ä¾‹3,269,017:1)
ä¿®å¤æ–¹æ¡ˆ: ä¿å®ˆå¹³è¡¡åˆå§‹åŒ– + æƒé‡çº¦æŸ + ç›‘æ§è¯Šæ–­

å‚è€ƒ: "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics"
"""

import torch
import torch.nn as nn
import numpy as np
import math
from typing import Dict, List, Optional, Tuple


class FixedKendallUncertainty(nn.Module):
    """
    ä¿®å¤ç‰ˆKendallä¸ç¡®å®šæ€§æƒé‡å­¦ä¹ å™¨
    è§£å†³æƒé‡ä¸¥é‡å¤±è¡¡é—®é¢˜ï¼Œå®ç°å¹³è¡¡çš„å¤šä»»åŠ¡å­¦ä¹ 
    """

    def __init__(self,
                 initial_pose_log_var: float = -1.0,
                 initial_detection_log_var: float = 0.0,
                 initial_gate_log_var: float = -0.4,
                 enable_weight_constraints: bool = True,
                 min_log_var: float = -2.0,
                 max_log_var: float = 2.0,
                 learning_rate_scale: float = 0.1):
        """
        Args:
            initial_pose_log_var: ä½å§¿ä»»åŠ¡åˆå§‹logæ–¹å·® (-1.0 â†’ Ïƒâ‰ˆ0.61, weightâ‰ˆ2.7)
            initial_detection_log_var: æ£€æµ‹ä»»åŠ¡åˆå§‹logæ–¹å·® (0.0 â†’ Ïƒ=1.0, weight=1.0)
            initial_gate_log_var: é—¨æ§ä»»åŠ¡åˆå§‹logæ–¹å·® (-0.4 â†’ Ïƒâ‰ˆ0.82, weightâ‰ˆ1.5)
            enable_weight_constraints: æ˜¯å¦å¯ç”¨æƒé‡èŒƒå›´çº¦æŸ
            min_log_var: logæ–¹å·®æœ€å°å€¼ (é˜²æ­¢æƒé‡è¿‡å¤§)
            max_log_var: logæ–¹å·®æœ€å¤§å€¼ (é˜²æ­¢æƒé‡è¿‡å°)
            learning_rate_scale: Kendallå‚æ•°å­¦ä¹ ç‡ç¼©æ”¾å› å­
        """
        super().__init__()

        # å¯å­¦ä¹ çš„logæ–¹å·®å‚æ•° (ä¿å®ˆå¹³è¡¡åˆå§‹åŒ–)
        self.pose_log_var = nn.Parameter(torch.tensor(initial_pose_log_var, dtype=torch.float32))
        self.detection_log_var = nn.Parameter(torch.tensor(initial_detection_log_var, dtype=torch.float32))
        self.gate_log_var = nn.Parameter(torch.tensor(initial_gate_log_var, dtype=torch.float32))

        # çº¦æŸå‚æ•°
        self.enable_constraints = enable_weight_constraints
        self.min_log_var = min_log_var
        self.max_log_var = max_log_var
        self.lr_scale = learning_rate_scale

        # ç»Ÿè®¡ä¿¡æ¯
        self.register_buffer('update_count', torch.tensor(0))
        self.register_buffer('weight_history', torch.zeros(100, 3))  # è®°å½•æœ€è¿‘100æ¬¡æƒé‡

        print(f"ğŸ”§ Fixed Kendall Uncertainty initialized:")
        print(f"   Pose: log_var={initial_pose_log_var:.2f} â†’ Ïƒ={np.exp(initial_pose_log_var/2):.3f}, weightâ‰ˆ{1/np.exp(initial_pose_log_var):.2f}")
        print(f"   Detection: log_var={initial_detection_log_var:.2f} â†’ Ïƒ={np.exp(initial_detection_log_var/2):.3f}, weightâ‰ˆ{1/np.exp(initial_detection_log_var):.2f}")
        print(f"   Gate: log_var={initial_gate_log_var:.2f} â†’ Ïƒ={np.exp(initial_gate_log_var/2):.3f}, weightâ‰ˆ{1/np.exp(initial_gate_log_var):.2f}")

    def apply_constraints(self):
        """åº”ç”¨log_varèŒƒå›´çº¦æŸ"""
        if self.enable_constraints:
            with torch.no_grad():
                self.pose_log_var.clamp_(self.min_log_var, self.max_log_var)
                self.detection_log_var.clamp_(self.min_log_var, self.max_log_var)
                self.gate_log_var.clamp_(self.min_log_var, self.max_log_var)

    def get_weights_and_sigmas(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—å½“å‰æƒé‡å’Œä¸ç¡®å®šæ€§
        Returns:
            weights: (pose_weight, detection_weight, gate_weight)
            sigmas: (pose_sigma, detection_sigma, gate_sigma)
            log_vars: (pose_log_var, detection_log_var, gate_log_var)
        """
        # åº”ç”¨çº¦æŸ
        self.apply_constraints()

        # è®¡ç®—Ïƒ = exp(log_var / 2)
        pose_sigma = torch.exp(self.pose_log_var / 2)
        detection_sigma = torch.exp(self.detection_log_var / 2)
        gate_sigma = torch.exp(self.gate_log_var / 2)

        # è®¡ç®—æƒé‡ = 1 / ÏƒÂ²
        pose_weight = 1.0 / (pose_sigma ** 2)
        detection_weight = 1.0 / (detection_sigma ** 2)
        gate_weight = 1.0 / (gate_sigma ** 2)

        weights = torch.stack([pose_weight, detection_weight, gate_weight])
        sigmas = torch.stack([pose_sigma, detection_sigma, gate_sigma])
        log_vars = torch.stack([self.pose_log_var, self.detection_log_var, self.gate_log_var])

        # æ›´æ–°å†å²è®°å½•
        self._update_history(weights)

        return weights, sigmas, log_vars

    def _update_history(self, weights: torch.Tensor):
        """æ›´æ–°æƒé‡å†å²è®°å½•"""
        with torch.no_grad():
            idx = self.update_count % 100
            self.weight_history[idx] = weights.detach()
            self.update_count += 1

    def compute_multitask_loss(self,
                              pose_loss: torch.Tensor,
                              detection_loss: torch.Tensor,
                              gate_loss: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—å¤šä»»åŠ¡æŸå¤±
        Args:
            pose_loss: ä½å§¿ä»»åŠ¡æŸå¤±
            detection_loss: æ£€æµ‹ä»»åŠ¡æŸå¤±
            gate_loss: é—¨æ§ä»»åŠ¡æŸå¤±
        Returns:
            åŒ…å«æ€»æŸå¤±å’Œå„ç»„ä»¶çš„å­—å…¸
        """
        weights, sigmas, log_vars = self.get_weights_and_sigmas()

        pose_weight, detection_weight, gate_weight = weights
        pose_sigma, detection_sigma, gate_sigma = sigmas

        # Kendallå¤šä»»åŠ¡æŸå¤±å…¬å¼: L = (1/ÏƒÂ²)*loss + log(Ïƒ)
        weighted_pose_loss = pose_weight * pose_loss + self.pose_log_var / 2
        weighted_detection_loss = detection_weight * detection_loss + self.detection_log_var / 2
        weighted_gate_loss = gate_weight * gate_loss + self.gate_log_var / 2

        total_loss = weighted_pose_loss + weighted_detection_loss + weighted_gate_loss

        return {
            'total_loss': total_loss,
            'weighted_pose_loss': weighted_pose_loss,
            'weighted_detection_loss': weighted_detection_loss,
            'weighted_gate_loss': weighted_gate_loss,
            'pose_weight': pose_weight,
            'detection_weight': detection_weight,
            'gate_weight': gate_weight,
            'pose_sigma': pose_sigma,
            'detection_sigma': detection_sigma,
            'gate_sigma': gate_sigma,
            'pose_log_var': self.pose_log_var,
            'detection_log_var': self.detection_log_var,
            'gate_log_var': self.gate_log_var
        }

    def forward(self, losses: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        å…¼å®¹åŸæœ‰æ¥å£çš„å‰å‘ä¼ æ’­

        Args:
            losses: å„ä»»åŠ¡æŸå¤±å­—å…¸
                - 'pose': å§¿æ€ä¼°è®¡æŸå¤±
                - 'detection': æ£€æµ‹æŸå¤±
                - 'gate': é—¨æ§æŸå¤±

        Returns:
            weighted_losses: åŠ æƒæŸå¤±å­—å…¸
        """
        pose_loss = losses.get('pose', torch.tensor(0.0, device=self.pose_log_var.device))
        detection_loss = losses.get('detection', torch.tensor(0.0, device=self.detection_log_var.device))
        gate_loss = losses.get('gate', torch.tensor(0.0, device=self.gate_log_var.device))

        result = self.compute_multitask_loss(pose_loss, detection_loss, gate_loss)

        # è½¬æ¢ä¸ºåŸæœ‰æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
        return {
            'total_loss': result['total_loss'],
            'weighted_pose': result['weighted_pose_loss'],
            'weighted_detection': result['weighted_detection_loss'],
            'weighted_gate': result['weighted_gate_loss'],
            'pose_weight': result['pose_weight'],
            'detection_weight': result['detection_weight'],
            'gate_weight': result['gate_weight'],
            'pose_sigma': result['pose_sigma'],
            'detection_sigma': result['detection_sigma'],
            'gate_sigma': result['gate_sigma']
        }

    def get_weights(self) -> Dict[str, float]:
        """è·å–å½“å‰æƒé‡å€¼ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰"""
        weights, sigmas, log_vars = self.get_weights_and_sigmas()

        return {
            'pose_weight': float(weights[0]),
            'detection_weight': float(weights[1]),
            'gate_weight': float(weights[2]),
            'pose_sigma': float(sigmas[0]),
            'detection_sigma': float(sigmas[1]),
            'gate_sigma': float(sigmas[2]),
            'pose_log_var': float(log_vars[0]),
            'detection_log_var': float(log_vars[1]),
            'gate_log_var': float(log_vars[2])
        }

    def get_weight_balance_metrics(self) -> Dict[str, float]:
        """è·å–æƒé‡å¹³è¡¡åˆ†ææŒ‡æ ‡"""
        weights, _, _ = self.get_weights_and_sigmas()

        total_weight = weights.sum()
        if total_weight == 0:
            return {'balance_score': 0.0, 'max_ratio': float('inf'), 'std_ratio': float('inf')}

        # æƒé‡æ¯”ä¾‹
        weight_ratios = weights / total_weight

        # å¹³è¡¡å¾—åˆ† (ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä»»åŠ¡æƒé‡åº”è¯¥ç›¸è¿‘)
        ideal_ratio = 1.0 / 3.0
        balance_score = 1.0 - torch.std(weight_ratios - ideal_ratio).item()

        # æœ€å¤§æƒé‡æ¯”ä¾‹
        max_ratio = torch.max(weights) / torch.min(weights)

        # æƒé‡æ ‡å‡†å·®æ¯”ä¾‹
        std_ratio = torch.std(weights) / torch.mean(weights)

        return {
            'balance_score': float(max(0, balance_score)),
            'max_ratio': float(max_ratio),
            'std_ratio': float(std_ratio),
            'pose_ratio': float(weight_ratios[0]),
            'detection_ratio': float(weight_ratios[1]),
            'gate_ratio': float(weight_ratios[2])
        }

    def get_optimization_suggestions(self) -> Dict[str, str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        balance_metrics = self.get_weight_balance_metrics()
        suggestions = {}

        if balance_metrics['max_ratio'] > 10:
            suggestions['severe_imbalance'] = f"æƒé‡æ¯”ä¾‹è¿‡å¤§ ({balance_metrics['max_ratio']:.1f}:1), å»ºè®®è°ƒæ•´åˆå§‹log_var"

        if balance_metrics['detection_ratio'] < 0.1:
            suggestions['detection_underweight'] = f"æ£€æµ‹ä»»åŠ¡æƒé‡è¿‡ä½ ({balance_metrics['detection_ratio']:.3f}), å»ºè®®é™ä½detection_log_var"

        if balance_metrics['pose_ratio'] > 0.7:
            suggestions['pose_overweight'] = f"ä½å§¿ä»»åŠ¡æƒé‡è¿‡é«˜ ({balance_metrics['pose_ratio']:.3f}), å»ºè®®æé«˜pose_log_var"

        if balance_metrics['balance_score'] < 0.5:
            suggestions['general_imbalance'] = f"æ•´ä½“æƒé‡ä¸å¹³è¡¡ (å¾—åˆ†{balance_metrics['balance_score']:.3f}), å»ºè®®é‡æ–°åˆå§‹åŒ–"

        return suggestions

    def print_status(self, epoch: int = None):
        """æ‰“å°å½“å‰çŠ¶æ€"""
        weights, sigmas, log_vars = self.get_weights_and_sigmas()
        balance_metrics = self.get_weight_balance_metrics()

        header = f"Kendall Status (Epoch {epoch})" if epoch is not None else "Kendall Status"
        print(f"\nğŸ“Š {header}:")
        print(f"   Weights: Pose={weights[0]:.2f}, Detection={weights[1]:.4f}, Gate={weights[2]:.2f}")
        print(f"   Sigmas:  Pose={sigmas[0]:.4f}, Detection={sigmas[1]:.4f}, Gate={sigmas[2]:.4f}")
        print(f"   LogVars: Pose={log_vars[0]:.3f}, Detection={log_vars[1]:.3f}, Gate={log_vars[2]:.3f}")
        print(f"   Balance: Score={balance_metrics['balance_score']:.3f}, MaxRatio={balance_metrics['max_ratio']:.1f}:1")

        # æ‰“å°å»ºè®®
        suggestions = self.get_optimization_suggestions()
        if suggestions:
            print(f"   âš ï¸ Suggestions:")
            for key, suggestion in suggestions.items():
                print(f"      - {suggestion}")

    def reset_parameters(self, initial_pose_log_var: float = -1.0,
                        initial_detection_log_var: float = 0.0,
                        initial_gate_log_var: float = -0.4):
        """é‡ç½®å‚æ•°åˆ°ä¿®å¤ç‰ˆåˆå§‹å€¼"""
        with torch.no_grad():
            self.pose_log_var.fill_(initial_pose_log_var)
            self.detection_log_var.fill_(initial_detection_log_var)
            self.gate_log_var.fill_(initial_gate_log_var)
            self.update_count.fill_(0)
            self.weight_history.zero_()


# åŸæœ‰ç±»çš„ä¿®å¤ç‰ˆåˆ«å (ä¿æŒå‘åå…¼å®¹)
class KendallUncertainty(FixedKendallUncertainty):
    """å‘åå…¼å®¹çš„åˆ«å"""
    def __init__(self, num_tasks: int = 3, init_log_var: float = 0.0,
                 min_log_var: float = -10.0, max_log_var: float = 5.0):
        # å°†æ—§å‚æ•°æ˜ å°„åˆ°æ–°çš„ä¿®å¤ç‰ˆå‚æ•°
        if init_log_var == 0.0 and min_log_var == -10.0 and max_log_var == 5.0:
            # ä½¿ç”¨ä¿®å¤ç‰ˆé»˜è®¤å€¼
            super().__init__(
                initial_pose_log_var=-1.0,
                initial_detection_log_var=0.0,
                initial_gate_log_var=-0.4,
                enable_weight_constraints=True,
                min_log_var=-2.0,
                max_log_var=2.0
            )
            print("âš ï¸ ä½¿ç”¨ä¿®å¤ç‰ˆKendallä¸ç¡®å®šæ€§ - æ—§æ¥å£å·²è‡ªåŠ¨è½¬æ¢ä¸ºå¹³è¡¡åˆå§‹åŒ–")
        else:
            # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å‚æ•°
            super().__init__(
                initial_pose_log_var=init_log_var,
                initial_detection_log_var=init_log_var,
                initial_gate_log_var=init_log_var,
                enable_weight_constraints=True,
                min_log_var=min_log_var,
                max_log_var=max_log_var
            )


def create_fixed_kendall_uncertainty(config: Dict = None) -> FixedKendallUncertainty:
    """åˆ›å»ºä¿®å¤ç‰ˆKendallä¸ç¡®å®šæ€§æ¨¡å—"""
    default_config = {
        'initial_pose_log_var': -1.0,      # ä¿å®ˆå¹³è¡¡
        'initial_detection_log_var': 0.0,   # åŸºå‡†
        'initial_gate_log_var': -0.4,      # é€‚ä¸­
        'enable_weight_constraints': True,
        'min_log_var': -2.0,
        'max_log_var': 2.0,
        'learning_rate_scale': 0.1
    }

    if config:
        default_config.update(config)

    return FixedKendallUncertainty(**default_config)


# å·¥å‚å‡½æ•° (æ›´æ–°ä¸ºä½¿ç”¨ä¿®å¤ç‰ˆ)
def create_kendall_uncertainty(uncertainty_type: str = 'fixed',
                              num_tasks: int = 3,
                              **kwargs) -> FixedKendallUncertainty:
    """
    åˆ›å»ºKendallä¸ç¡®å®šæ€§æ¨¡å— (ç°åœ¨é»˜è®¤ä½¿ç”¨ä¿®å¤ç‰ˆ)

    Args:
        uncertainty_type: 'fixed' (æ¨è) æˆ– 'basic' (å‘åå…¼å®¹)
        num_tasks: ä»»åŠ¡æ•°é‡ (å›ºå®šä¸º3)
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        uncertainty_module: ä¿®å¤ç‰ˆKendallä¸ç¡®å®šæ€§æ¨¡å—
    """
    if uncertainty_type == 'basic':
        # å‘åå…¼å®¹æ¨¡å¼
        return KendallUncertainty(num_tasks=num_tasks, **kwargs)
    else:
        # æ¨èçš„ä¿®å¤ç‰ˆ
        return create_fixed_kendall_uncertainty(kwargs)


# æ¨¡å—æµ‹è¯•
if __name__ == '__main__':
    print("ğŸ§ª Testing Fixed Kendall Uncertainty...")

    # æµ‹è¯•ä¿®å¤ç‰ˆKendallä¸ç¡®å®šæ€§
    print("\n=== ä¿®å¤ç‰ˆKendallä¸ç¡®å®šæ€§æµ‹è¯• ===")
    kendall = create_fixed_kendall_uncertainty()

    # æ¨¡æ‹Ÿå½“å‰é—®é¢˜çš„æŸå¤±å€¼
    pose_loss = torch.tensor(0.005)
    detection_loss = torch.tensor(1.8)
    gate_loss = torch.tensor(0.00001)

    print(f"\nè¾“å…¥æŸå¤±å€¼:")
    print(f"  Pose Loss: {pose_loss.item():.6f}")
    print(f"  Detection Loss: {detection_loss.item():.6f}")
    print(f"  Gate Loss: {gate_loss.item():.8f}")

    # è®¡ç®—å¤šä»»åŠ¡æŸå¤±
    result = kendall.compute_multitask_loss(pose_loss, detection_loss, gate_loss)

    print(f"\nä¿®å¤åçš„æŸå¤±è®¡ç®—:")
    print(f"  Total Loss: {result['total_loss']:.6f}")
    print(f"  Weighted Losses:")
    print(f"    Pose: {result['weighted_pose_loss']:.6f}")
    print(f"    Detection: {result['weighted_detection_loss']:.6f}")
    print(f"    Gate: {result['weighted_gate_loss']:.6f}")

    # æ˜¾ç¤ºæƒé‡çŠ¶æ€
    kendall.print_status()

    # æµ‹è¯•æƒé‡å¹³è¡¡æŒ‡æ ‡
    balance_metrics = kendall.get_weight_balance_metrics()
    print(f"\næƒé‡å¹³è¡¡åˆ†æ:")
    print(f"  å¹³è¡¡å¾—åˆ†: {balance_metrics['balance_score']:.3f}")
    print(f"  æœ€å¤§æ¯”ä¾‹: {balance_metrics['max_ratio']:.1f}:1")
    print(f"  æƒé‡åˆ†å¸ƒ: Pose={balance_metrics['pose_ratio']:.3f}, Detection={balance_metrics['detection_ratio']:.3f}, Gate={balance_metrics['gate_ratio']:.3f}")

    # æµ‹è¯•å‘åå…¼å®¹æ€§
    print(f"\n=== å‘åå…¼å®¹æ€§æµ‹è¯• ===")
    legacy_kendall = KendallUncertainty()

    # ä½¿ç”¨æ—§æ¥å£
    legacy_losses = {
        'pose': pose_loss,
        'detection': detection_loss,
        'gate': gate_loss
    }

    legacy_result = legacy_kendall(legacy_losses)
    print(f"æ—§æ¥å£æ€»æŸå¤±: {legacy_result['total_loss']:.6f}")

    # å¯¹æ¯”æµ‹è¯•ï¼šæ¨¡æ‹ŸåŸæœ‰é—®é¢˜é…ç½®
    print(f"\n=== é—®é¢˜é…ç½®å¯¹æ¯”æµ‹è¯• ===")
    problem_kendall = FixedKendallUncertainty(
        initial_pose_log_var=-10.0,  # åŸé—®é¢˜é…ç½®
        initial_detection_log_var=5.0,
        initial_gate_log_var=-10.0,
        enable_weight_constraints=False
    )

    problem_result = problem_kendall.compute_multitask_loss(pose_loss, detection_loss, gate_loss)
    problem_kendall.print_status()

    print(f"\nå¯¹æ¯”ç»“æœ:")
    print(f"  ä¿®å¤ç‰ˆæ€»æŸå¤±: {result['total_loss']:.6f}")
    print(f"  é—®é¢˜ç‰ˆæ€»æŸå¤±: {problem_result['total_loss']:.6f}")
    print(f"  æƒé‡æ¯”ä¾‹æ”¹å–„:")
    print(f"    ä¿®å¤ç‰ˆ: {result['pose_weight']:.2f} : {result['detection_weight']:.4f} : {result['gate_weight']:.2f}")
    print(f"    é—®é¢˜ç‰ˆ: {problem_result['pose_weight']:.0f} : {problem_result['detection_weight']:.6f} : {problem_result['gate_weight']:.0f}")

    print("\nâœ… Fixed Kendall Uncertainty test completed!")
    print("\nğŸ¯ å…³é”®æ”¹è¿›:")
    print("   1. æƒé‡æ¯”ä¾‹ä» 3,269,017:1 é™ä½åˆ° 2.7:1")
    print("   2. æ£€æµ‹ä»»åŠ¡é‡æ–°è·å¾—åˆç†æƒé‡")
    print("   3. æ·»åŠ æƒé‡èŒƒå›´çº¦æŸå’Œç›‘æ§")
    print("   4. ä¿æŒå‘åå…¼å®¹æ€§")