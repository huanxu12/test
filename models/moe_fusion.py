"""
MoEFusion: Mixture of Experts Fusion Module for MineSLAM
å¤šä¸“å®¶èåˆæ¨¡å—ï¼šGeometric/Semantic/Visualä¸“å®¶ + çƒ­å¼•å¯¼æœºåˆ¶
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass


@dataclass
class MoEConfig:
    """MoEé…ç½®ç±»"""
    embedding_dim: int = 512
    num_experts: int = 3
    num_encoder_layers: int = 2
    nhead: int = 4
    feedforward_dim: int = 2048
    dropout: float = 0.1
    gate_hidden_dim: int = 256
    thermal_guidance: bool = True
    gate_entropy_weight: float = 0.01


class ThermalGuidedAttention(nn.Module):
    """çƒ­å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, config: MoEConfig):
        super().__init__()
        self.config = config
        self.embedding_dim = config.embedding_dim
        self.nhead = config.nhead
        
        # æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›
        self.attention = nn.MultiheadAttention(
            embed_dim=self.embedding_dim,
            num_heads=self.nhead,
            dropout=config.dropout,
            batch_first=True
        )
        
        # çƒ­å¼•å¯¼å¤„ç†
        self.thermal_processor = nn.Sequential(
            nn.Linear(config.embedding_dim, config.embedding_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(config.embedding_dim // 2, 1),
            nn.Sigmoid()
        )
        
    def generate_thermal_mask(self, thermal_tokens: torch.Tensor, 
                            target_length: int) -> torch.Tensor:
        """
        ä»çƒ­æˆåƒtokenç”Ÿæˆç©ºé—´mask
        
        Args:
            thermal_tokens: [B, T_thermal, D] çƒ­æˆåƒtoken
            target_length: ç›®æ ‡åºåˆ—é•¿åº¦
            
        Returns:
            mask: [B, target_length] ç©ºé—´mask
        """
        batch_size = thermal_tokens.shape[0]
        
        # ç”Ÿæˆçƒ­é‡è¦æ€§åˆ†æ•°
        thermal_importance = self.thermal_processor(thermal_tokens)  # [B, T_thermal, 1]
        thermal_importance = thermal_importance.squeeze(-1)  # [B, T_thermal]
        
        # ä¸Šé‡‡æ ·åˆ°ç›®æ ‡é•¿åº¦
        if thermal_tokens.shape[1] != target_length:
            # ä½¿ç”¨çº¿æ€§æ’å€¼ä¸Šé‡‡æ ·
            thermal_importance = thermal_importance.unsqueeze(1)  # [B, 1, T_thermal]
            mask = F.interpolate(
                thermal_importance, 
                size=target_length, 
                mode='linear', 
                align_corners=False
            )
            mask = mask.squeeze(1)  # [B, target_length]
        else:
            mask = thermal_importance
        
        return mask
    
    def apply_thermal_guidance(self, attention_weights: torch.Tensor,
                             thermal_mask: torch.Tensor,
                             guidance_type: str = 'additive') -> torch.Tensor:
        """
        åº”ç”¨çƒ­å¼•å¯¼åˆ°æ³¨æ„åŠ›æƒé‡
        
        Args:
            attention_weights: [B, T, T] æ³¨æ„åŠ›æƒé‡ (averaged across heads)
            thermal_mask: [B, T] çƒ­å¼•å¯¼mask
            guidance_type: 'additive' æˆ– 'multiplicative'
            
        Returns:
            guided_weights: [B, T, T] å¼•å¯¼åçš„æ³¨æ„åŠ›æƒé‡
        """
        if attention_weights.dim() == 4:
            # [B, H, T, T] format - average across heads
            attention_weights = attention_weights.mean(dim=1)
        
        batch_size, seq_len, _ = attention_weights.shape
        
        # ç”Ÿæˆmask bias [B, T, T]
        mask_bias = thermal_mask.unsqueeze(-1).expand(-1, -1, seq_len)  # [B, T, T]
        
        if guidance_type == 'additive':
            # åŠ æ€§bias
            guided_weights = attention_weights + mask_bias * 0.1  # ç¼©æ”¾å› å­
        elif guidance_type == 'multiplicative':
            # ä¹˜æ€§bias
            guided_weights = attention_weights * (1.0 + mask_bias)
        else:
            raise ValueError(f"Unknown guidance type: {guidance_type}")
        
        # é‡æ–°å½’ä¸€åŒ–
        guided_weights = F.softmax(guided_weights, dim=-1)
        
        return guided_weights
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                thermal_tokens: Optional[torch.Tensor] = None,
                guidance_type: str = 'additive') -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            query, key, value: [B, T, D] æ³¨æ„åŠ›è¾“å…¥
            thermal_tokens: [B, T_thermal, D] çƒ­æˆåƒtokenï¼ˆå¯é€‰ï¼‰
            guidance_type: çƒ­å¼•å¯¼ç±»å‹
            
        Returns:
            output: [B, T, D] è¾“å‡º
            attention_weights: [B, T, T] æ³¨æ„åŠ›æƒé‡
        """
        # æ ‡å‡†æ³¨æ„åŠ›
        output, attention_weights = self.attention(query, key, value)
        
        # å¦‚æœæœ‰çƒ­æˆåƒå¼•å¯¼
        if thermal_tokens is not None and self.config.thermal_guidance:
            # ç”Ÿæˆçƒ­å¼•å¯¼mask
            thermal_mask = self.generate_thermal_mask(thermal_tokens, query.shape[1])
            
            # åº”ç”¨çƒ­å¼•å¯¼
            guided_weights = self.apply_thermal_guidance(
                attention_weights, thermal_mask, guidance_type
            )
            
            # é‡æ–°è®¡ç®—è¾“å‡ºä½¿ç”¨å¼•å¯¼åçš„æƒé‡
            # æ³¨æ„ï¼šPyTorchçš„attention_weightsæ˜¯[B, T, T]æ ¼å¼ï¼Œä¸æ˜¯[B, H, T, T]
            guided_output = torch.matmul(guided_weights, value)  # [B, T, D]
            
            return guided_output, guided_weights
        
        return output, attention_weights


class ExpertLayer(nn.Module):
    """ä¸“å®¶å±‚ï¼šTransformer Encoder Layer with Thermal Guidance"""
    
    def __init__(self, config: MoEConfig, expert_type: str = 'generic'):
        super().__init__()
        self.config = config
        self.expert_type = expert_type
        self.embedding_dim = config.embedding_dim
        
        # çƒ­å¼•å¯¼æ³¨æ„åŠ›ï¼ˆä»…Semanticä¸“å®¶ä½¿ç”¨ï¼‰
        if expert_type == 'semantic':
            self.self_attention = ThermalGuidedAttention(config)
        else:
            self.self_attention = nn.MultiheadAttention(
                embed_dim=config.embedding_dim,
                num_heads=config.nhead,
                dropout=config.dropout,
                batch_first=True
            )
        
        # å‰é¦ˆç½‘ç»œ
        self.feedforward = nn.Sequential(
            nn.Linear(config.embedding_dim, config.feedforward_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(config.dropout),
            nn.Linear(config.feedforward_dim, config.embedding_dim),
            nn.Dropout(config.dropout)
        )
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(config.embedding_dim)
        self.norm2 = nn.LayerNorm(config.embedding_dim)
        
        # Dropout
        self.dropout = nn.Dropout(config.dropout)
        
        print(f"Expert Layer initialized: {expert_type.upper()}")
    
    def forward(self, x: torch.Tensor, 
                thermal_tokens: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [B, T, D] è¾“å…¥tokenåºåˆ—
            thermal_tokens: [B, T_thermal, D] çƒ­æˆåƒtokenï¼ˆç”¨äºSemanticä¸“å®¶ï¼‰
            
        Returns:
            output: [B, T, D] è¾“å‡º
            attention_weights: [B, H, T, T] æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        """
        # Self-attention
        if self.expert_type == 'semantic' and thermal_tokens is not None:
            # Semanticä¸“å®¶ä½¿ç”¨çƒ­å¼•å¯¼æ³¨æ„åŠ›
            attn_output, attention_weights = self.self_attention(
                x, x, x, thermal_tokens=thermal_tokens
            )
        else:
            # å…¶ä»–ä¸“å®¶ä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›
            attn_output, attention_weights = self.self_attention(x, x, x)
        
        # æ®‹å·®è¿æ¥ + LayerNorm
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ
        ff_output = self.feedforward(x)
        
        # æ®‹å·®è¿æ¥ + LayerNorm
        output = self.norm2(x + ff_output)
        
        return {
            'output': output,
            'attention_weights': attention_weights
        }


class Expert(nn.Module):
    """å•ä¸ªä¸“å®¶ï¼šå¤šå±‚Encoder"""
    
    def __init__(self, config: MoEConfig, expert_type: str):
        super().__init__()
        self.expert_type = expert_type
        self.config = config
        
        # å¤šå±‚Encoder
        self.layers = nn.ModuleList([
            ExpertLayer(config, expert_type) 
            for _ in range(config.num_encoder_layers)
        ])
        
        print(f"{expert_type.upper()} Expert initialized: {config.num_encoder_layers} layers")
    
    def forward(self, x: torch.Tensor, 
                thermal_tokens: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [B, T, D] è¾“å…¥tokenåºåˆ—
            thermal_tokens: [B, T_thermal, D] çƒ­æˆåƒtoken
            
        Returns:
            output: [B, T, D] ä¸“å®¶è¾“å‡º
            attention_maps: List[Tensor] å„å±‚æ³¨æ„åŠ›æƒé‡
        """
        attention_maps = []
        
        for layer in self.layers:
            result = layer(x, thermal_tokens)
            x = result['output']
            attention_maps.append(result['attention_weights'])
        
        return {
            'output': x,
            'attention_maps': attention_maps
        }


class GatingNetwork(nn.Module):
    """é—¨æ§ç½‘ç»œ"""
    
    def __init__(self, config: MoEConfig):
        super().__init__()
        self.config = config
        
        # é—¨æ§MLP
        self.gate_network = nn.Sequential(
            nn.Linear(config.embedding_dim, config.gate_hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(config.dropout),
            nn.Linear(config.gate_hidden_dim, config.gate_hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(config.dropout),
            nn.Linear(config.gate_hidden_dim // 2, config.num_experts)
        )
        
        print(f"Gating Network initialized: {config.embedding_dim} â†’ {config.num_experts} experts")
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [B, T, D] è¾“å…¥tokenåºåˆ—
            
        Returns:
            gate_weights: [B, T, num_experts] é—¨æ§æƒé‡
            gate_entropy: [B] é—¨æ§ç†µï¼ˆç”¨äºæ­£åˆ™åŒ–ï¼‰
        """
        # è®¡ç®—é—¨æ§åˆ†æ•°
        gate_logits = self.gate_network(x)  # [B, T, num_experts]
        
        # Softmaxå½’ä¸€åŒ–
        gate_weights = F.softmax(gate_logits, dim=-1)
        
        # è®¡ç®—ç†µï¼ˆç”¨äºæ­£åˆ™åŒ–ï¼‰
        gate_entropy = -torch.sum(
            gate_weights * torch.log(gate_weights + 1e-8), 
            dim=-1
        )  # [B, T]
        gate_entropy = gate_entropy.mean(dim=1)  # [B]
        
        return {
            'gate_weights': gate_weights,
            'gate_entropy': gate_entropy,
            'gate_logits': gate_logits
        }


class MoEFusion(nn.Module):
    """
    MoEèåˆæ¨¡å—
    åŒ…å«ä¸‰ä¸ªä¸“å®¶ï¼šGeometric, Semantic, Visual
    """
    
    def __init__(self, config: MoEConfig):
        super().__init__()
        self.config = config
        
        # ä¸‰ä¸ªä¸“å®¶
        self.experts = nn.ModuleDict({
            'geometric': Expert(config, 'geometric'),
            'semantic': Expert(config, 'semantic'),  # ä½¿ç”¨çƒ­å¼•å¯¼
            'visual': Expert(config, 'visual')
        })
        
        # é—¨æ§ç½‘ç»œ
        self.gating_network = GatingNetwork(config)
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(config.embedding_dim, config.embedding_dim)
        
        print(f"MoEFusion initialized: {config.num_experts} experts, thermal_guidance={config.thermal_guidance}")
    
    def compute_gate_entropy_loss(self, gate_entropy: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—é—¨æ§ç†µæŸå¤±ï¼ˆè´Ÿç†µï¼Œé˜²æ­¢ä¸“å®¶å¡Œç¼©ï¼‰
        
        Args:
            gate_entropy: [B] é—¨æ§ç†µ
            
        Returns:
            entropy_loss: scalar ç†µæŸå¤±
        """
        # è´Ÿç†µæŸå¤±ï¼ˆé¼“åŠ±å‡åŒ€åˆ†å¸ƒï¼‰
        max_entropy = math.log(self.config.num_experts)  # å‡åŒ€åˆ†å¸ƒçš„æœ€å¤§ç†µ
        entropy_loss = max_entropy - gate_entropy.mean()  # è´Ÿç†µ
        
        return entropy_loss * self.config.gate_entropy_weight
    
    def forward(self, token_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            token_dict: å„æ¨¡æ€tokenå­—å…¸
                - 'rgb': [B, T_rgb, D]
                - 'depth': [B, T_depth, D]
                - 'thermal': [B, T_thermal, D]
                - 'lidar': [B, T_lidar, D]
                - 'imu': [B, T_imu, D]
                
        Returns:
            result_dict: åŒ…å«èåˆç»“æœå’Œä¸­é—´ä¿¡æ¯
        """
        # æ‹¼æ¥æ‰€æœ‰token
        all_tokens = []
        modality_boundaries = {}
        current_idx = 0
        
        for modality, tokens in token_dict.items():
            all_tokens.append(tokens)
            modality_boundaries[modality] = (current_idx, current_idx + tokens.shape[1])
            current_idx += tokens.shape[1]
        
        # æ‹¼æ¥ï¼š[B, Total_T, D]
        fused_tokens = torch.cat(all_tokens, dim=1)
        batch_size, total_tokens, embedding_dim = fused_tokens.shape
        
        # é—¨æ§å†³ç­–
        gate_result = self.gating_network(fused_tokens)
        gate_weights = gate_result['gate_weights']  # [B, Total_T, 3]
        gate_entropy = gate_result['gate_entropy']  # [B]
        
        # ä¸“å®¶å¤„ç†
        expert_outputs = {}
        expert_attention_maps = {}
        
        for expert_name, expert in self.experts.items():
            if expert_name == 'semantic' and 'thermal' in token_dict:
                # Semanticä¸“å®¶ä½¿ç”¨çƒ­å¼•å¯¼
                expert_result = expert(fused_tokens, token_dict['thermal'])
            else:
                # å…¶ä»–ä¸“å®¶æ ‡å‡†å¤„ç†
                expert_result = expert(fused_tokens)
            
            expert_outputs[expert_name] = expert_result['output']
            expert_attention_maps[expert_name] = expert_result['attention_maps']
        
        # ä¸“å®¶è¾“å‡ºåŠ æƒèåˆ
        expert_names = ['geometric', 'semantic', 'visual']
        weighted_outputs = []
        
        for i, expert_name in enumerate(expert_names):
            expert_output = expert_outputs[expert_name]  # [B, Total_T, D]
            expert_weight = gate_weights[:, :, i:i+1]  # [B, Total_T, 1]
            weighted_output = expert_output * expert_weight
            weighted_outputs.append(weighted_output)
        
        # èåˆæ‰€æœ‰ä¸“å®¶è¾“å‡º
        fused_output = sum(weighted_outputs)  # [B, Total_T, D]
        
        # è¾“å‡ºæŠ•å½±
        final_output = self.output_projection(fused_output)
        
        # è®¡ç®—é—¨æ§ç†µæŸå¤±
        entropy_loss = self.compute_gate_entropy_loss(gate_entropy)
        
        # åˆ†è§£å›å„æ¨¡æ€
        output_tokens = {}
        for modality, (start_idx, end_idx) in modality_boundaries.items():
            output_tokens[modality] = final_output[:, start_idx:end_idx, :]
        
        return {
            'fused_tokens': output_tokens,
            'gate_weights': gate_weights,
            'gate_entropy': gate_entropy,
            'entropy_loss': entropy_loss,
            'expert_outputs': expert_outputs,
            'expert_attention_maps': expert_attention_maps,
            'modality_boundaries': modality_boundaries
        }


def create_moe_fusion(embedding_dim: int = 512, **kwargs) -> MoEFusion:
    """
    åˆ›å»ºMoEèåˆæ¨¡å—çš„ä¾¿æ·å‡½æ•°
    
    Args:
        embedding_dim: åµŒå…¥ç»´åº¦
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        MoEFusionæ¨¡å—
    """
    config = MoEConfig(embedding_dim=embedding_dim, **kwargs)
    return MoEFusion(config)


# æ¨¡å—æµ‹è¯•
if __name__ == '__main__':
    # æµ‹è¯•é…ç½®
    config = MoEConfig(
        embedding_dim=512,
        num_experts=3,
        num_encoder_layers=2,
        nhead=4,
        thermal_guidance=True
    )
    
    # åˆ›å»ºæ¨¡å—
    moe_fusion = MoEFusion(config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    test_tokens = {
        'rgb': torch.randn(batch_size, 192, 512),
        'depth': torch.randn(batch_size, 192, 512),
        'thermal': torch.randn(batch_size, 192, 512),
        'lidar': torch.randn(batch_size, 1, 512),
        'imu': torch.randn(batch_size, 1, 512),
    }
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        result = moe_fusion(test_tokens)
    
    print("\nğŸ§ª MoEFusion Test Results:")
    print(f"âœ“ Input total tokens: {sum(t.shape[1] for t in test_tokens.values())}")
    print(f"âœ“ Gate weights shape: {result['gate_weights'].shape}")
    print(f"âœ“ Gate entropy: {result['gate_entropy'].mean().item():.4f}")
    print(f"âœ“ Entropy loss: {result['entropy_loss'].item():.6f}")
    print(f"âœ“ Expert outputs: {list(result['expert_outputs'].keys())}")
    print(f"âœ“ Fused tokens: {list(result['fused_tokens'].keys())}")
    
    print("\nğŸ‰ MoEFusion module test passed!")