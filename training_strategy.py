"""
Advanced Training Strategy for MineSLAM
é›†æˆè‡ªåŠ¨æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ã€å­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœç­‰å…ˆè¿›è®­ç»ƒç­–ç•¥
"""

import os
import sys
import math
import time
import warnings
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
from torch.optim.lr_scheduler import _LRScheduler


class CosineLRWithWarmup(_LRScheduler):
    """
    Cosine Annealing LR with Linear Warmup
    å…ˆçº¿æ€§é¢„çƒ­ï¼Œå†ä½™å¼¦é€€ç«
    """

    def __init__(self, optimizer, warmup_steps: int, total_steps: int,
                 eta_min: float = 0.0, last_epoch: int = -1):
        """
        Args:
            optimizer: ä¼˜åŒ–å™¨
            warmup_steps: é¢„çƒ­æ­¥æ•°
            total_steps: æ€»è®­ç»ƒæ­¥æ•°
            eta_min: æœ€å°å­¦ä¹ ç‡
            last_epoch: ä¸Šæ¬¡epochç¼–å·
        """
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.eta_min = eta_min

        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        if self.last_epoch == 0:
            return [0.0 for _ in self.base_lrs]

        if self.last_epoch < self.warmup_steps:
            # çº¿æ€§é¢„çƒ­é˜¶æ®µ
            return [base_lr * self.last_epoch / self.warmup_steps
                    for base_lr in self.base_lrs]
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            cosine_steps = self.total_steps - self.warmup_steps
            current_cosine_step = self.last_epoch - self.warmup_steps

            return [self.eta_min + (base_lr - self.eta_min) *
                    (1 + math.cos(math.pi * current_cosine_step / cosine_steps)) / 2
                    for base_lr in self.base_lrs]


class EarlyStopping:
    """
    Early Stopping with Multiple Metrics
    ç›‘æ§å¤šä¸ªæŒ‡æ ‡çš„æ—©åœæœºåˆ¶
    """

    def __init__(self, patience: int = 10, min_delta: float = 1e-4,
                 restore_best_weights: bool = True, mode: str = 'min'):
        """
        Args:
            patience: å®¹å¿çš„æ— æ”¹å–„epochæ•°
            min_delta: æœ€å°æ”¹å–„é˜ˆå€¼
            restore_best_weights: æ˜¯å¦æ¢å¤æœ€ä½³æƒé‡
            mode: 'min' æˆ– 'max'
        """
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.mode = mode

        self.best_score = None
        self.counter = 0
        self.best_state = None
        self.early_stop = False

    def __call__(self, score: float, model: nn.Module) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ—©åœ

        Args:
            score: å½“å‰åˆ†æ•°
            model: æ¨¡å‹ï¼ˆç”¨äºä¿å­˜æœ€ä½³çŠ¶æ€ï¼‰

        Returns:
            æ˜¯å¦åº”è¯¥æ—©åœ
        """
        if self.best_score is None:
            self.best_score = score
            self._save_checkpoint(model)
        elif self._is_improved(score):
            self.best_score = score
            self.counter = 0
            self._save_checkpoint(model)
        else:
            self.counter += 1

        if self.counter >= self.patience:
            self.early_stop = True
            if self.restore_best_weights and self.best_state is not None:
                model.load_state_dict(self.best_state)
                print(f"âœ… Restored best weights (score: {self.best_score:.6f})")

        return self.early_stop

    def _is_improved(self, score: float) -> bool:
        """åˆ¤æ–­åˆ†æ•°æ˜¯å¦æ”¹å–„"""
        if self.mode == 'min':
            return score < self.best_score - self.min_delta
        else:
            return score > self.best_score + self.min_delta

    def _save_checkpoint(self, model: nn.Module):
        """ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€"""
        if self.restore_best_weights:
            self.best_state = model.state_dict().copy()


class GradientAccumulator:
    """
    Gradient Accumulation Manager
    æ¢¯åº¦ç´¯ç§¯ç®¡ç†å™¨
    """

    def __init__(self, accumulation_steps: int = 1, clip_grad_norm: float = 1.0):
        """
        Args:
            accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            clip_grad_norm: æ¢¯åº¦è£å‰ªé˜ˆå€¼
        """
        self.accumulation_steps = accumulation_steps
        self.clip_grad_norm = clip_grad_norm
        self.current_step = 0

    def should_step(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤"""
        self.current_step += 1
        return self.current_step % self.accumulation_steps == 0

    def scale_loss(self, loss: torch.Tensor) -> torch.Tensor:
        """ç¼©æ”¾æŸå¤±ç”¨äºæ¢¯åº¦ç´¯ç§¯"""
        return loss / self.accumulation_steps

    def clip_gradients(self, model: nn.Module) -> float:
        """æ¢¯åº¦è£å‰ª"""
        if self.clip_grad_norm > 0:
            grad_norm = torch.nn.utils.clip_grad_norm_(
                model.parameters(), self.clip_grad_norm
            )
            return grad_norm.item()
        return 0.0

    def reset(self):
        """é‡ç½®æ­¥æ•°è®¡æ•°"""
        self.current_step = 0


class AMPManager:
    """
    Automatic Mixed Precision Manager
    è‡ªåŠ¨æ··åˆç²¾åº¦ç®¡ç†å™¨
    """

    def __init__(self, enabled: bool = True, init_scale: float = 2**16,
                 growth_factor: float = 2.0, backoff_factor: float = 0.5,
                 growth_interval: int = 2000):
        """
        Args:
            enabled: æ˜¯å¦å¯ç”¨AMP
            init_scale: åˆå§‹ç¼©æ”¾å› å­
            growth_factor: å¢é•¿å› å­
            backoff_factor: å›é€€å› å­
            growth_interval: å¢é•¿é—´éš”
        """
        self.enabled = enabled
        self.scaler = None

        if self.enabled and torch.cuda.is_available():
            self.scaler = GradScaler(
                init_scale=init_scale,
                growth_factor=growth_factor,
                backoff_factor=backoff_factor,
                growth_interval=growth_interval
            )
            print(f"âœ… AMP enabled with init_scale={init_scale}")
        else:
            print("âŒ AMP disabled (CUDA not available or manually disabled)")

    def autocast_context(self):
        """è¿”å›autocastä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return autocast(enabled=self.enabled)

    def scale_loss(self, loss: torch.Tensor) -> torch.Tensor:
        """ç¼©æ”¾æŸå¤±"""
        if self.enabled and self.scaler is not None:
            return self.scaler.scale(loss)
        return loss

    def unscale_gradients(self, optimizer: optim.Optimizer):
        """åç¼©æ”¾æ¢¯åº¦ï¼ˆç”¨äºæ¢¯åº¦è£å‰ªï¼‰"""
        if self.enabled and self.scaler is not None:
            self.scaler.unscale_(optimizer)

    def step_optimizer(self, optimizer: optim.Optimizer):
        """æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤"""
        if self.enabled and self.scaler is not None:
            self.scaler.step(optimizer)
            self.scaler.update()
        else:
            optimizer.step()

    def get_scale(self) -> float:
        """è·å–å½“å‰ç¼©æ”¾å› å­"""
        if self.enabled and self.scaler is not None:
            return self.scaler.get_scale()
        return 1.0


class TrainingStrategy:
    """
    Integrated Training Strategy
    é›†æˆè®­ç»ƒç­–ç•¥ç®¡ç†å™¨
    """

    def __init__(self, config: Dict):
        """
        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
        """
        self.config = config

        # è®­ç»ƒå‚æ•°
        self.max_epochs = config.get('max_epochs', 100)
        self.learning_rate = config.get('learning_rate', 1e-4)
        self.weight_decay = config.get('weight_decay', 1e-5)

        # æ¢¯åº¦ç´¯ç§¯
        accumulation_config = config.get('gradient_accumulation', {})
        self.accumulator = GradientAccumulator(
            accumulation_steps=accumulation_config.get('steps', 4),
            clip_grad_norm=accumulation_config.get('clip_norm', 1.0)
        )

        # AMPè®¾ç½®
        amp_config = config.get('amp', {})
        self.amp_manager = AMPManager(
            enabled=amp_config.get('enabled', True),
            init_scale=amp_config.get('init_scale', 2**16)
        )

        # æ—©åœè®¾ç½®
        early_stop_config = config.get('early_stopping', {})
        self.early_stopping = EarlyStopping(
            patience=early_stop_config.get('patience', 10),
            min_delta=early_stop_config.get('min_delta', 1e-4),
            restore_best_weights=early_stop_config.get('restore_best_weights', True),
            mode=early_stop_config.get('mode', 'min')
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆåœ¨create_schedulerä¸­åˆå§‹åŒ–ï¼‰
        self.scheduler = None

        print(f"TrainingStrategy initialized:")
        print(f"  Max epochs: {self.max_epochs}")
        print(f"  Learning rate: {self.learning_rate}")
        print(f"  Gradient accumulation: {self.accumulator.accumulation_steps} steps")
        print(f"  AMP enabled: {self.amp_manager.enabled}")
        print(f"  Early stopping patience: {self.early_stopping.patience}")

    def create_optimizer(self, model: nn.Module) -> optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_config = self.config.get('optimizer', {})
        optimizer_type = optimizer_config.get('type', 'adamw')

        if optimizer_type.lower() == 'adamw':
            optimizer = optim.AdamW(
                model.parameters(),
                lr=self.learning_rate,
                weight_decay=self.weight_decay,
                betas=optimizer_config.get('betas', (0.9, 0.999)),
                eps=optimizer_config.get('eps', 1e-8)
            )
        elif optimizer_type.lower() == 'adam':
            optimizer = optim.Adam(
                model.parameters(),
                lr=self.learning_rate,
                weight_decay=self.weight_decay,
                betas=optimizer_config.get('betas', (0.9, 0.999))
            )
        else:
            raise ValueError(f"Unsupported optimizer type: {optimizer_type}")

        print(f"âœ… Created {optimizer_type.upper()} optimizer")
        return optimizer

    def create_scheduler(self, optimizer: optim.Optimizer,
                        total_steps: int) -> _LRScheduler:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_config = self.config.get('scheduler', {})
        scheduler_type = scheduler_config.get('type', 'cosine_warmup')

        if scheduler_type == 'cosine_warmup':
            warmup_steps = scheduler_config.get('warmup_steps', 1000)
            eta_min = scheduler_config.get('eta_min', self.learning_rate * 0.01)

            self.scheduler = CosineLRWithWarmup(
                optimizer=optimizer,
                warmup_steps=warmup_steps,
                total_steps=total_steps,
                eta_min=eta_min
            )
            print(f"âœ… Created Cosine LR with Warmup scheduler (warmup: {warmup_steps} steps)")

        elif scheduler_type == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=total_steps,
                eta_min=scheduler_config.get('eta_min', self.learning_rate * 0.01)
            )
            print(f"âœ… Created Cosine Annealing LR scheduler")

        elif scheduler_type == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                optimizer,
                step_size=scheduler_config.get('step_size', total_steps // 3),
                gamma=scheduler_config.get('gamma', 0.1)
            )
            print(f"âœ… Created Step LR scheduler")

        else:
            raise ValueError(f"Unsupported scheduler type: {scheduler_type}")

        return self.scheduler

    def forward_pass(self, model: nn.Module, inputs: Dict[str, torch.Tensor]) -> Any:
        """å¸¦AMPçš„å‰å‘ä¼ æ’­"""
        with self.amp_manager.autocast_context():
            outputs = model(inputs)
        return outputs

    def backward_pass(self, loss: torch.Tensor, model: nn.Module,
                     optimizer: optim.Optimizer) -> Dict[str, float]:
        """å¸¦æ¢¯åº¦ç´¯ç§¯å’ŒAMPçš„åå‘ä¼ æ’­"""
        # ç¼©æ”¾æŸå¤±ç”¨äºæ¢¯åº¦ç´¯ç§¯
        scaled_loss = self.accumulator.scale_loss(loss)

        # AMPç¼©æ”¾æŸå¤±
        scaled_loss = self.amp_manager.scale_loss(scaled_loss)

        # åå‘ä¼ æ’­
        scaled_loss.backward()

        metrics = {'loss': loss.item(), 'scaled_loss': scaled_loss.item()}

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
        if self.accumulator.should_step():
            # åç¼©æ”¾æ¢¯åº¦ï¼ˆç”¨äºæ¢¯åº¦è£å‰ªï¼‰
            self.amp_manager.unscale_gradients(optimizer)

            # æ¢¯åº¦è£å‰ª
            grad_norm = self.accumulator.clip_gradients(model)
            metrics['grad_norm'] = grad_norm

            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.amp_manager.step_optimizer(optimizer)
            optimizer.zero_grad()

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                self.scheduler.step()

            metrics['lr'] = optimizer.param_groups[0]['lr']
            metrics['amp_scale'] = self.amp_manager.get_scale()

        return metrics

    def check_early_stopping(self, score: float, model: nn.Module) -> bool:
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        return self.early_stopping(score, model)

    def get_training_state(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºä¿å­˜æ£€æŸ¥ç‚¹ï¼‰"""
        state = {
            'accumulator_step': self.accumulator.current_step,
            'early_stopping_counter': self.early_stopping.counter,
            'early_stopping_best_score': self.early_stopping.best_score,
        }

        if self.scheduler is not None:
            state['scheduler_state'] = self.scheduler.state_dict()

        if self.amp_manager.scaler is not None:
            state['scaler_state'] = self.amp_manager.scaler.state_dict()

        return state

    def load_training_state(self, state: Dict[str, Any],
                           scheduler: _LRScheduler = None):
        """åŠ è½½è®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºæ¢å¤æ£€æŸ¥ç‚¹ï¼‰"""
        self.accumulator.current_step = state.get('accumulator_step', 0)
        self.early_stopping.counter = state.get('early_stopping_counter', 0)
        self.early_stopping.best_score = state.get('early_stopping_best_score', None)

        if 'scheduler_state' in state and scheduler is not None:
            scheduler.load_state_dict(state['scheduler_state'])

        if 'scaler_state' in state and self.amp_manager.scaler is not None:
            self.amp_manager.scaler.load_state_dict(state['scaler_state'])


def create_training_strategy(config: Dict) -> TrainingStrategy:
    """
    Factory function to create training strategy

    Args:
        config: è®­ç»ƒé…ç½®

    Returns:
        é…ç½®å¥½çš„è®­ç»ƒç­–ç•¥
    """
    return TrainingStrategy(config)


# ç¤ºä¾‹é…ç½®
EXAMPLE_CONFIG = {
    'max_epochs': 100,
    'learning_rate': 1e-4,
    'weight_decay': 1e-5,

    'gradient_accumulation': {
        'steps': 4,
        'clip_norm': 1.0
    },

    'amp': {
        'enabled': True,
        'init_scale': 2**16
    },

    'early_stopping': {
        'patience': 10,
        'min_delta': 1e-4,
        'restore_best_weights': True,
        'mode': 'min'
    },

    'optimizer': {
        'type': 'adamw',
        'betas': (0.9, 0.999),
        'eps': 1e-8
    },

    'scheduler': {
        'type': 'cosine_warmup',
        'warmup_steps': 1000,
        'eta_min': 1e-6
    }
}


if __name__ == '__main__':
    print("ğŸ§ª Testing Training Strategy...")

    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹
    model = torch.nn.Linear(10, 1)

    # åˆ›å»ºè®­ç»ƒç­–ç•¥
    strategy = create_training_strategy(EXAMPLE_CONFIG)

    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = strategy.create_optimizer(model)
    scheduler = strategy.create_scheduler(optimizer, total_steps=5000)

    print(f"\nâœ… Training strategy test completed!")
    print(f"Optimizer: {type(optimizer).__name__}")
    print(f"Scheduler: {type(scheduler).__name__}")
    print(f"AMP enabled: {strategy.amp_manager.enabled}")
    print(f"Gradient accumulation: {strategy.accumulator.accumulation_steps} steps")