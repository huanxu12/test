"""
SLAM Evaluation Dataset Extension
æ‰©å±•ç°æœ‰MineSLAMDatasetä»¥æ”¯æŒå®Œæ•´çš„SLAMè¯„ä¼°ï¼šground truthåŠ è½½ã€DARPAæ ‡æ³¨ã€è½¨è¿¹åˆ†æ
"""

import os
import json
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
import cv2
from PIL import Image
from typing import Dict, List, Tuple, Optional, Any
import bisect
from pathlib import Path
import warnings
from scipy.spatial.transform import Rotation as R

# å¯¼å…¥ç°æœ‰æ•°æ®é›†
from data.mineslam_dataset import MineSLAMDataset, RealDataContract


class SLAMGroundTruthLoader:
    """SLAMçœŸå€¼æ•°æ®åŠ è½½å™¨"""

    def __init__(self, data_root: str):
        self.data_root = Path(data_root)
        self.gt_trajectory_file = self.data_root / "ground_truth_trajectory.csv"
        self.gt_stats_file = self.data_root / "ground_truth_stats.json"
        self.artifacts_file = self.data_root / "sr_B_route2.bag.artifacts"
        self.fiducial_file = self.data_root / "fiducial_sr"

        # åŠ è½½çœŸå€¼æ•°æ®
        self.ground_truth_poses = self._load_ground_truth_trajectory()
        self.ground_truth_stats = self._load_ground_truth_stats()
        self.artifact_annotations = self._load_artifact_annotations()
        self.fiducial_positions = self._load_fiducial_positions()

        print(f"ğŸ“ Ground truth loaded: {len(self.ground_truth_poses)} poses")

    def _load_ground_truth_trajectory(self) -> pd.DataFrame:
        """åŠ è½½çœŸå€¼è½¨è¿¹æ•°æ®"""
        if not self.gt_trajectory_file.exists():
            warnings.warn(f"Ground truth trajectory file not found: {self.gt_trajectory_file}")
            return pd.DataFrame()

        try:
            # è¯»å–CSVæ–‡ä»¶
            gt_df = pd.read_csv(self.gt_trajectory_file)

            # éªŒè¯å¿…éœ€çš„åˆ—
            required_cols = ['timestamp', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw']
            missing_cols = [col for col in required_cols if col not in gt_df.columns]

            if missing_cols:
                warnings.warn(f"Missing columns in ground truth: {missing_cols}")
                return pd.DataFrame()

            # æ’åºå¹¶å»é‡
            gt_df = gt_df.sort_values('timestamp').drop_duplicates('timestamp').reset_index(drop=True)

            print(f"âœ… Loaded {len(gt_df)} ground truth poses")
            print(f"   Time range: {gt_df['timestamp'].min():.2f} - {gt_df['timestamp'].max():.2f}s")
            print(f"   Trajectory length: {self._compute_trajectory_length(gt_df):.2f}m")

            return gt_df

        except Exception as e:
            warnings.warn(f"Error loading ground truth trajectory: {e}")
            return pd.DataFrame()

    def _compute_trajectory_length(self, gt_df: pd.DataFrame) -> float:
        """è®¡ç®—è½¨è¿¹æ€»é•¿åº¦"""
        if len(gt_df) < 2:
            return 0.0

        positions = gt_df[['x', 'y', 'z']].values
        distances = np.linalg.norm(np.diff(positions, axis=0), axis=1)
        return float(np.sum(distances))

    def _load_ground_truth_stats(self) -> Dict[str, Any]:
        """åŠ è½½çœŸå€¼ç»Ÿè®¡ä¿¡æ¯"""
        if not self.gt_stats_file.exists():
            warnings.warn(f"Ground truth stats file not found: {self.gt_stats_file}")
            return {}

        try:
            with open(self.gt_stats_file, 'r') as f:
                stats = json.load(f)
            print(f"ğŸ“Š Ground truth stats: {stats}")
            return stats
        except Exception as e:
            warnings.warn(f"Error loading ground truth stats: {e}")
            return {}

    def _load_artifact_annotations(self) -> List[Dict[str, Any]]:
        """åŠ è½½DARPAç‰©ä½“æ ‡æ³¨"""
        if not self.artifacts_file.exists():
            warnings.warn(f"Artifacts file not found: {self.artifacts_file}")
            return []

        try:
            artifacts = []
            with open(self.artifacts_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        # è§£ææ ‡æ³¨æ ¼å¼ï¼ˆå…·ä½“æ ¼å¼éœ€è¦æ ¹æ®å®é™…æ–‡ä»¶ç¡®å®šï¼‰
                        parts = line.split(',')
                        if len(parts) >= 4:
                            artifact = {
                                'class_name': parts[0].strip(),
                                'x': float(parts[1]),
                                'y': float(parts[2]),
                                'z': float(parts[3]),
                                'confidence': float(parts[4]) if len(parts) > 4 else 1.0
                            }
                            artifacts.append(artifact)

            print(f"ğŸ·ï¸ Loaded {len(artifacts)} artifact annotations")
            return artifacts

        except Exception as e:
            warnings.warn(f"Error loading artifact annotations: {e}")
            return []

    def _load_fiducial_positions(self) -> List[Dict[str, Any]]:
        """åŠ è½½åŸºå‡†æ ‡å¿—ä½ç½®"""
        if not self.fiducial_file.exists():
            warnings.warn(f"Fiducial file not found: {self.fiducial_file}")
            return []

        try:
            fiducials = []
            with open(self.fiducial_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        parts = line.split()
                        if len(parts) >= 3:
                            fiducial = {
                                'id': len(fiducials),
                                'x': float(parts[0]),
                                'y': float(parts[1]),
                                'z': float(parts[2])
                            }
                            fiducials.append(fiducial)

            print(f"ğŸ“ Loaded {len(fiducials)} fiducial markers")
            return fiducials

        except Exception as e:
            warnings.warn(f"Error loading fiducial positions: {e}")
            return []

    def get_pose_at_timestamp(self, timestamp: float) -> Optional[np.ndarray]:
        """è·å–æŒ‡å®šæ—¶é—´æˆ³çš„ä½å§¿"""
        if self.ground_truth_poses.empty:
            return None

        # æ‰¾åˆ°æœ€è¿‘çš„æ—¶é—´æˆ³
        timestamps = self.ground_truth_poses['timestamp'].values
        idx = np.argmin(np.abs(timestamps - timestamp))

        # æ£€æŸ¥æ—¶é—´å·®æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        time_diff = abs(timestamps[idx] - timestamp)
        if time_diff > 0.1:  # 100mså®¹å·®
            warnings.warn(f"Large time difference: {time_diff:.3f}s for timestamp {timestamp}")

        # æå–7ç»´ä½å§¿ [x,y,z,qx,qy,qz,qw]
        pose_row = self.ground_truth_poses.iloc[idx]
        pose = np.array([
            pose_row['x'], pose_row['y'], pose_row['z'],
            pose_row['qx'], pose_row['qy'], pose_row['qz'], pose_row['qw']
        ])

        return pose

    def get_artifacts_in_view(self, pose: np.ndarray, max_distance: float = 10.0) -> List[Dict[str, Any]]:
        """è·å–è§†é‡å†…çš„æ ‡æ³¨ç‰©ä½“"""
        if not self.artifact_annotations:
            return []

        pose_position = pose[:3]
        visible_artifacts = []

        for artifact in self.artifact_annotations:
            artifact_position = np.array([artifact['x'], artifact['y'], artifact['z']])
            distance = np.linalg.norm(pose_position - artifact_position)

            if distance <= max_distance:
                artifact_copy = artifact.copy()
                artifact_copy['distance'] = distance
                visible_artifacts.append(artifact_copy)

        return visible_artifacts


class DARPAClassMapping:
    """DARPA SubTæŒ‘æˆ˜èµ›ç±»åˆ«æ˜ å°„"""

    # DARPA 8ç±»ç‰©ä½“æ˜ å°„
    CLASS_NAMES = [
        'fiducial',      # 0 - åŸºå‡†æ ‡å¿—
        'extinguisher',  # 1 - ç­ç«å™¨
        'phone',         # 2 - ç”µè¯
        'backpack',      # 3 - èƒŒåŒ…
        'survivor',      # 4 - å¹¸å­˜è€…
        'drill',         # 5 - é’»å¤´
        'rope',          # 6 - ç»³ç´¢ï¼ˆå¦‚æœæœ‰ï¼‰
        'other'          # 7 - å…¶ä»–ç‰©ä½“
    ]

    CLASS_TO_ID = {name: idx for idx, name in enumerate(CLASS_NAMES)}
    ID_TO_CLASS = {idx: name for idx, name in enumerate(CLASS_NAMES)}

    @classmethod
    def get_class_id(cls, class_name: str) -> int:
        """è·å–ç±»åˆ«ID"""
        # å¤„ç†å„ç§å¯èƒ½çš„å‘½åå˜ä½“
        class_name = class_name.lower().strip()

        # ç›´æ¥åŒ¹é…
        if class_name in cls.CLASS_TO_ID:
            return cls.CLASS_TO_ID[class_name]

        # æ¨¡ç³ŠåŒ¹é…
        if 'fiducial' in class_name or 'marker' in class_name:
            return 0
        elif 'extinguisher' in class_name or 'fire' in class_name:
            return 1
        elif 'phone' in class_name or 'cellphone' in class_name:
            return 2
        elif 'backpack' in class_name or 'bag' in class_name:
            return 3
        elif 'survivor' in class_name or 'person' in class_name or 'human' in class_name:
            return 4
        elif 'drill' in class_name or 'tool' in class_name:
            return 5
        elif 'rope' in class_name or 'cable' in class_name:
            return 6
        else:
            return 7  # other

    @classmethod
    def get_class_name(cls, class_id: int) -> str:
        """è·å–ç±»åˆ«åç§°"""
        return cls.ID_TO_CLASS.get(class_id, 'unknown')


class SLAMEvaluationDataset(MineSLAMDataset):
    """æ‰©å±•çš„SLAMè¯„ä¼°æ•°æ®é›†"""

    def __init__(self, data_root: str, split: str = "train", sequence_length: int = 1,
                 load_ground_truth: bool = True, **kwargs):
        """
        åˆå§‹åŒ–SLAMè¯„ä¼°æ•°æ®é›†

        Args:
            data_root: æ•°æ®æ ¹ç›®å½•
            split: æ•°æ®åˆ†å‰² ("train", "val", "test")
            sequence_length: åºåˆ—é•¿åº¦
            load_ground_truth: æ˜¯å¦åŠ è½½çœŸå€¼æ•°æ®
        """
        super().__init__(data_root, split, sequence_length, **kwargs)

        self.load_ground_truth = load_ground_truth

        # åŠ è½½çœŸå€¼æ•°æ®
        if self.load_ground_truth:
            self.gt_loader = SLAMGroundTruthLoader(data_root)
        else:
            self.gt_loader = None

        # DARPAç±»åˆ«æ˜ å°„
        self.class_mapping = DARPAClassMapping()

        print(f"ğŸ” SLAM Evaluation Dataset initialized:")
        print(f"   Split: {split}")
        print(f"   Samples: {len(self)}")
        print(f"   Ground truth: {'Enabled' if load_ground_truth else 'Disabled'}")

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """è·å–å¢å¼ºçš„è¯„ä¼°æ ·æœ¬"""
        # è·å–åŸºç¡€æ ·æœ¬
        sample = super().__getitem__(idx)

        # æ·»åŠ SLAMè¯„ä¼°ç›¸å…³ä¿¡æ¯
        if self.load_ground_truth and self.gt_loader:
            sample = self._add_ground_truth_info(sample, idx)

        return sample

    def _add_ground_truth_info(self, sample: Dict[str, Any], idx: int) -> Dict[str, Any]:
        """æ·»åŠ çœŸå€¼ä¿¡æ¯åˆ°æ ·æœ¬"""
        # è·å–æ—¶é—´æˆ³
        timestamp = sample.get('timestamp', 0.0)

        # è·å–çœŸå€¼ä½å§¿
        gt_pose = self.gt_loader.get_pose_at_timestamp(timestamp)
        if gt_pose is not None:
            sample['gt_pose'] = torch.from_numpy(gt_pose).float()
        else:
            # åˆ›å»ºå•ä½ä½å§¿ä½œä¸ºé»˜è®¤å€¼
            sample['gt_pose'] = torch.tensor([0, 0, 0, 0, 0, 0, 1], dtype=torch.float32)

        # è·å–è§†é‡å†…çš„æ ‡æ³¨ç‰©ä½“
        if gt_pose is not None:
            visible_artifacts = self.gt_loader.get_artifacts_in_view(gt_pose)
            sample['gt_detections'] = self._process_artifacts_to_detections(visible_artifacts)
        else:
            sample['gt_detections'] = {
                'boxes': torch.zeros(0, 6),  # (N, 6) - 3Dè¾¹ç•Œæ¡†
                'classes': torch.zeros(0, dtype=torch.long),  # (N,) - ç±»åˆ«
                'scores': torch.zeros(0)  # (N,) - ç½®ä¿¡åº¦
            }

        # æ·»åŠ è¯„ä¼°å…ƒæ•°æ®
        sample['eval_metadata'] = {
            'sample_idx': idx,
            'timestamp': timestamp,
            'has_ground_truth': gt_pose is not None,
            'num_visible_artifacts': len(visible_artifacts) if gt_pose is not None else 0
        }

        return sample

    def _process_artifacts_to_detections(self, artifacts: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        """å°†æ ‡æ³¨ç‰©ä½“è½¬æ¢ä¸ºæ£€æµ‹æ ¼å¼"""
        if not artifacts:
            return {
                'boxes': torch.zeros(0, 6),
                'classes': torch.zeros(0, dtype=torch.long),
                'scores': torch.zeros(0)
            }

        boxes = []
        classes = []
        scores = []

        for artifact in artifacts:
            # 3Dè¾¹ç•Œæ¡† (ä¸­å¿ƒä½ç½® + å°ºå¯¸ï¼Œç®€åŒ–å¤„ç†)
            center = [artifact['x'], artifact['y'], artifact['z']]
            size = [1.0, 1.0, 1.0]  # é»˜è®¤å°ºå¯¸ï¼Œå®é™…åº”è¯¥æ ¹æ®ç‰©ä½“ç±»åˆ«è°ƒæ•´
            box = center + size
            boxes.append(box)

            # ç±»åˆ«ID
            class_id = self.class_mapping.get_class_id(artifact['class_name'])
            classes.append(class_id)

            # ç½®ä¿¡åº¦
            scores.append(artifact.get('confidence', 1.0))

        return {
            'boxes': torch.tensor(boxes, dtype=torch.float32),  # (N, 6)
            'classes': torch.tensor(classes, dtype=torch.long),  # (N,)
            'scores': torch.tensor(scores, dtype=torch.float32)  # (N,)
        }

    def get_trajectory_sequence(self, start_idx: int, length: int) -> List[Dict[str, Any]]:
        """è·å–è½¨è¿¹åºåˆ—ç”¨äºè¯„ä¼°"""
        trajectory = []

        for i in range(start_idx, min(start_idx + length, len(self))):
            sample = self[i]
            trajectory.append(sample)

        return trajectory

    def get_evaluation_statistics(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_samples': len(self),
            'ground_truth_available': self.load_ground_truth,
            'class_distribution': defaultdict(int),
            'trajectory_coverage': 0.0
        }

        if self.load_ground_truth and self.gt_loader:
            # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
            for artifact in self.gt_loader.artifact_annotations:
                class_name = artifact['class_name']
                class_id = self.class_mapping.get_class_id(class_name)
                stats['class_distribution'][self.class_mapping.get_class_name(class_id)] += 1

            # è½¨è¿¹è¦†ç›–åº¦
            if not self.gt_loader.ground_truth_poses.empty:
                total_time = (self.gt_loader.ground_truth_poses['timestamp'].max() -
                            self.gt_loader.ground_truth_poses['timestamp'].min())
                stats['trajectory_coverage'] = total_time
                stats['trajectory_length'] = self.gt_loader._compute_trajectory_length(
                    self.gt_loader.ground_truth_poses
                )

        return dict(stats)


def create_slam_evaluation_splits(data_root: str, train_ratio: float = 0.7,
                                val_ratio: float = 0.15, test_ratio: float = 0.15,
                                save_splits: bool = True) -> Dict[str, List[int]]:
    """
    åˆ›å»ºSLAMè¯„ä¼°æ•°æ®åˆ†å‰²

    Args:
        data_root: æ•°æ®æ ¹ç›®å½•
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        save_splits: æ˜¯å¦ä¿å­˜åˆ†å‰²ä¿¡æ¯

    Returns:
        åŒ…å«å„åˆ†å‰²ç´¢å¼•çš„å­—å…¸
    """
    # åˆ›å»ºä¸´æ—¶æ•°æ®é›†ä»¥è·å–æ€»æ ·æœ¬æ•°
    temp_dataset = SLAMEvaluationDataset(data_root, split="train", load_ground_truth=False)
    total_samples = len(temp_dataset)

    # è®¡ç®—åˆ†å‰²å¤§å°
    train_size = int(total_samples * train_ratio)
    val_size = int(total_samples * val_ratio)
    test_size = total_samples - train_size - val_size

    # åˆ›å»ºéšæœºåˆ†å‰²
    np.random.seed(42)  # å›ºå®šç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
    indices = np.random.permutation(total_samples)

    splits = {
        'train': indices[:train_size].tolist(),
        'val': indices[train_size:train_size + val_size].tolist(),
        'test': indices[train_size + val_size:].tolist()
    }

    print(f"ğŸ“Š Created SLAM evaluation splits:")
    print(f"   Train: {len(splits['train'])} samples ({train_ratio:.1%})")
    print(f"   Val: {len(splits['val'])} samples ({val_ratio:.1%})")
    print(f"   Test: {len(splits['test'])} samples ({test_ratio:.1%})")

    # ä¿å­˜åˆ†å‰²ä¿¡æ¯
    if save_splits:
        splits_file = Path(data_root) / "slam_evaluation_splits.json"
        with open(splits_file, 'w') as f:
            json.dump(splits, f, indent=2)
        print(f"ğŸ’¾ Splits saved to: {splits_file}")

    return splits


def load_slam_evaluation_splits(data_root: str) -> Optional[Dict[str, List[int]]]:
    """åŠ è½½å·²ä¿å­˜çš„SLAMè¯„ä¼°åˆ†å‰²"""
    splits_file = Path(data_root) / "slam_evaluation_splits.json"

    if not splits_file.exists():
        return None

    try:
        with open(splits_file, 'r') as f:
            splits = json.load(f)
        print(f"ğŸ“‚ Loaded SLAM evaluation splits from: {splits_file}")
        return splits
    except Exception as e:
        warnings.warn(f"Error loading splits: {e}")
        return None


class SLAMDatasetFactory:
    """SLAMæ•°æ®é›†å·¥å‚ç±»"""

    @staticmethod
    def create_evaluation_dataset(data_root: str, split: str = "train",
                                **kwargs) -> SLAMEvaluationDataset:
        """åˆ›å»ºSLAMè¯„ä¼°æ•°æ®é›†"""
        return SLAMEvaluationDataset(data_root, split=split, **kwargs)

    @staticmethod
    def create_training_dataset(data_root: str, **kwargs) -> SLAMEvaluationDataset:
        """åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆå«çœŸå€¼ï¼‰"""
        return SLAMEvaluationDataset(data_root, split="train", load_ground_truth=True, **kwargs)

    @staticmethod
    def create_validation_dataset(data_root: str, **kwargs) -> SLAMEvaluationDataset:
        """åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆå«çœŸå€¼ï¼‰"""
        return SLAMEvaluationDataset(data_root, split="val", load_ground_truth=True, **kwargs)

    @staticmethod
    def create_test_dataset(data_root: str, **kwargs) -> SLAMEvaluationDataset:
        """åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆå«çœŸå€¼ï¼‰"""
        return SLAMEvaluationDataset(data_root, split="test", load_ground_truth=True, **kwargs)


if __name__ == "__main__":
    # æµ‹è¯•SLAMè¯„ä¼°æ•°æ®é›†
    print("ğŸ§ª SLAM Evaluation Dataset - Test Mode")

    data_root = "sr_B_route2_deep_learning"

    try:
        # æµ‹è¯•çœŸå€¼åŠ è½½å™¨
        gt_loader = SLAMGroundTruthLoader(data_root)

        # æµ‹è¯•æ•°æ®é›†
        eval_dataset = SLAMEvaluationDataset(data_root, split="train", load_ground_truth=True)

        # è·å–æ ·æœ¬
        sample = eval_dataset[0]
        print(f"ğŸ“¦ Sample keys: {list(sample.keys())}")

        if 'gt_pose' in sample:
            print(f"ğŸ¯ Ground truth pose shape: {sample['gt_pose'].shape}")

        if 'gt_detections' in sample:
            gt_det = sample['gt_detections']
            print(f"ğŸ·ï¸ Ground truth detections:")
            print(f"   Boxes: {gt_det['boxes'].shape}")
            print(f"   Classes: {gt_det['classes'].shape}")

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = eval_dataset.get_evaluation_statistics()
        print(f"ğŸ“Š Dataset statistics: {stats}")

        print("âœ… SLAM evaluation dataset test passed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()