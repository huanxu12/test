"""
Enhanced Training Loop with SLAM Evaluation Integration
é›†æˆSLAMè¯„ä¼°åŠŸèƒ½çš„å¢å¼ºè®­ç»ƒå¾ªç¯ï¼šå®æ—¶æŒ‡æ ‡ç›‘æ§ã€å¯è§†åŒ–ã€å®Œæ•´è¯„ä¼°æŠ¥å‘Š
"""

import os
import sys
import json
import time
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler
try:
    from torch.amp import autocast
except ImportError:
    from torch.cuda.amp import autocast
from torch.utils.data import DataLoader, Subset

# å¯¼å…¥ç°æœ‰æ¨¡å—
from train_loop import TrainingLoop, pointcloud_collate_fn
from slam_evaluator import MineSLAMEvaluator
from slam_visualizer import MineSLAMVisualizer
from slam_evaluation_dataset import SLAMEvaluationDataset, SLAMDatasetFactory


class EnhancedSLAMTrainingLoop(TrainingLoop):
    """å¢å¼ºçš„SLAMè®­ç»ƒå¾ªç¯ - é›†æˆè¯„ä¼°å’Œå¯è§†åŒ–"""

    def __init__(self, config: Dict[str, Any], *args, **kwargs):
        """
        åˆå§‹åŒ–å¢å¼ºçš„SLAMè®­ç»ƒå¾ªç¯

        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
            *args, **kwargs: ä¼ é€’ç»™çˆ¶ç±»çš„å‚æ•°
        """
        super().__init__(config, *args, **kwargs)

        # SLAMè¯„ä¼°é…ç½®
        self.slam_config = config.get('slam_evaluation', {})
        self.enable_slam_eval = self.slam_config.get('enabled', True)
        self.eval_frequency = self.slam_config.get('eval_frequency', 5)  # æ¯5ä¸ªepochè¯„ä¼°ä¸€æ¬¡
        self.visualize_frequency = self.slam_config.get('visualize_frequency', 10)
        self.save_visualizations = self.slam_config.get('save_visualizations', True)

        # åˆå§‹åŒ–SLAMè¯„ä¼°å™¨
        if self.enable_slam_eval:
            self.slam_evaluator = MineSLAMEvaluator(self.model, self.device)
            print("ğŸ” SLAM Evaluator initialized")
        else:
            self.slam_evaluator = None

        # åˆå§‹åŒ–å¯è§†åŒ–å™¨
        if self.save_visualizations:
            viz_output_dir = os.path.join(self.output_dir, "slam_visualization")
            self.slam_visualizer = MineSLAMVisualizer(viz_output_dir)
            print(f"ğŸ¨ SLAM Visualizer initialized: {viz_output_dir}")
        else:
            self.slam_visualizer = None

        # SLAMè¯„ä¼°å†å²
        self.slam_metrics_history = []
        self.best_slam_metrics = {
            'best_ate': float('inf'),
            'best_map': 0.0,
            'best_combined_score': 0.0,
            'best_epoch': 0
        }

        print(f"ğŸš€ Enhanced SLAM Training Loop initialized")
        print(f"   SLAM Evaluation: {'Enabled' if self.enable_slam_eval else 'Disabled'}")
        print(f"   Visualization: {'Enabled' if self.save_visualizations else 'Disabled'}")

    def setup_slam_evaluation_datasets(self):
        """è®¾ç½®SLAMè¯„ä¼°æ•°æ®é›†"""
        try:
            # ä½¿ç”¨SLAMè¯„ä¼°æ•°æ®é›†æ›¿æ¢ç°æœ‰æ•°æ®é›†
            if hasattr(self, 'val_dataset'):
                data_root = self.val_dataset.data_root

                # åˆ›å»ºSLAMè¯„ä¼°éªŒè¯æ•°æ®é›†
                self.slam_val_dataset = SLAMDatasetFactory.create_validation_dataset(
                    data_root,
                    sequence_length=self.val_dataset.sequence_length
                )

                print(f"ğŸ”„ Replaced validation dataset with SLAM evaluation dataset")
                print(f"   Samples: {len(self.slam_val_dataset)}")

                # è·å–æ•°æ®é›†ç»Ÿè®¡
                stats = self.slam_val_dataset.get_evaluation_statistics()
                print(f"ğŸ“Š SLAM Dataset Stats: {stats}")

        except Exception as e:
            warnings.warn(f"Failed to setup SLAM evaluation datasets: {e}")
            self.slam_val_dataset = None

    def validation_epoch(self, epoch: int) -> Dict[str, float]:
        """å¢å¼ºçš„éªŒè¯å‡½æ•° - é›†æˆSLAMæŒ‡æ ‡"""
        print(f"\nğŸ” Enhanced Validation - Epoch {epoch}")

        # æ‰§è¡ŒåŸæœ‰éªŒè¯
        standard_metrics = super().validation_epoch(epoch)

        # SLAMç‰¹å®šè¯„ä¼°
        slam_metrics = {}
        if self.enable_slam_eval and self.slam_evaluator and epoch % self.eval_frequency == 0:
            slam_metrics = self._run_slam_evaluation(epoch)

        # å¯è§†åŒ–æ›´æ–°
        if self.slam_visualizer and epoch % self.visualize_frequency == 0:
            self._update_visualizations(epoch, standard_metrics, slam_metrics)

        # åˆå¹¶æŒ‡æ ‡
        combined_metrics = {**standard_metrics, **slam_metrics}

        # æ›´æ–°æœ€ä½³SLAMæŒ‡æ ‡
        if slam_metrics:
            self._update_best_slam_metrics(epoch, slam_metrics)

        return combined_metrics

    def _run_slam_evaluation(self, epoch: int) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„SLAMè¯„ä¼°"""
        print(f"ğŸ¯ Running SLAM evaluation at epoch {epoch}")

        if not hasattr(self, 'slam_val_dataset') or self.slam_val_dataset is None:
            self.setup_slam_evaluation_datasets()

        if self.slam_val_dataset is None:
            warnings.warn("SLAM validation dataset not available")
            return {}

        try:
            # é‡ç½®è¯„ä¼°å™¨
            self.slam_evaluator.reset_all_metrics()

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            slam_val_loader = DataLoader(
                self.slam_val_dataset,
                batch_size=min(4, self.batch_size),  # å‡å°æ‰¹æ¬¡ä»¥èŠ‚çœå†…å­˜
                shuffle=False,
                collate_fn=pointcloud_collate_fn,
                num_workers=2
            )

            # è¯„ä¼°å¾ªç¯
            self.model.eval()
            total_samples = 0

            with torch.no_grad():
                for batch_idx, batch in enumerate(slam_val_loader):
                    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                    batch = self._move_batch_to_device(batch)

                    # æå–çœŸå€¼æ•°æ®
                    gt_poses = batch.get('gt_pose', torch.zeros(batch['rgb'].shape[0], 7)).cpu().numpy()
                    gt_detections = {
                        'boxes': batch['gt_detections']['boxes'] if 'gt_detections' in batch else torch.zeros(0, 6),
                        'classes': batch['gt_detections']['classes'] if 'gt_detections' in batch else torch.zeros(0, dtype=torch.long)
                    }

                    # è¿è¡Œè¯„ä¼°
                    batch_metrics = self.slam_evaluator.evaluate_batch(batch, gt_poses, gt_detections)
                    total_samples += batch['rgb'].shape[0]

                    # è¿›åº¦è¾“å‡º
                    if batch_idx % 20 == 0:
                        print(f"   Processed {total_samples} samples...")

            # è·å–æœ€ç»ˆæŒ‡æ ‡
            final_metrics = self.slam_evaluator.get_current_metrics()

            # æ·»åŠ å‰ç¼€ä»¥åŒºåˆ†SLAMæŒ‡æ ‡
            slam_metrics = {}
            for category, metrics in final_metrics.items():
                if isinstance(metrics, dict):
                    for metric_name, value in metrics.items():
                        slam_key = f"slam_{category}_{metric_name}"
                        slam_metrics[slam_key] = value
                else:
                    slam_key = f"slam_{category}"
                    slam_metrics[slam_key] = metrics

            # è®°å½•å†å²
            self.slam_metrics_history.append({
                'epoch': epoch,
                'metrics': slam_metrics.copy()
            })

            print(f"âœ… SLAM evaluation completed:")
            print(f"   ATE: {slam_metrics.get('slam_trajectory_metrics_ATE', 0):.4f}m")
            print(f"   mAP: {slam_metrics.get('slam_detection_metrics_mAP', 0):.4f}")

            return slam_metrics

        except Exception as e:
            warnings.warn(f"SLAM evaluation failed: {e}")
            import traceback
            traceback.print_exc()
            return {}

    def _move_batch_to_device(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡"""
        device_batch = {}

        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                device_batch[key] = value.to(self.device)
            elif isinstance(value, dict):
                device_batch[key] = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                                   for k, v in value.items()}
            else:
                device_batch[key] = value

        return device_batch

    def _update_visualizations(self, epoch: int, standard_metrics: Dict[str, float],
                             slam_metrics: Dict[str, Any]):
        """æ›´æ–°å¯è§†åŒ–"""
        try:
            # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡ç”¨äºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
            all_metrics = {**standard_metrics, **slam_metrics}

            # é‡æ–°ç»„ç»‡æŒ‡æ ‡æ ¼å¼ä»¥é€‚é…å¯è§†åŒ–å™¨
            viz_metrics = {
                'losses': {
                    'pose': standard_metrics.get('val_pose_loss', 0),
                    'detection': standard_metrics.get('val_detection_loss', 0),
                    'total': standard_metrics.get('val_total_loss', 0)
                },
                'trajectory_metrics': {
                    'ATE': slam_metrics.get('slam_trajectory_metrics_ATE', 0)
                },
                'detection_metrics': {
                    'mAP': slam_metrics.get('slam_detection_metrics_mAP', 0)
                },
                'kendall_weights': {
                    'pose_weight': getattr(self.model, 'last_pose_weight', 0),
                    'detection_weight': getattr(self.model, 'last_detection_weight', 0),
                    'gate_weight': getattr(self.model, 'last_gate_weight', 0)
                }
            }

            # æ›´æ–°è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
            self.slam_visualizer.update_training_progress(epoch, viz_metrics)

            # ä¿å­˜å¯è§†åŒ–
            if epoch % (self.visualize_frequency * 2) == 0:  # å‡å°‘ä¿å­˜é¢‘ç‡
                self.slam_visualizer.save_all_visualizations(epoch, "enhanced_slam")

        except Exception as e:
            warnings.warn(f"Visualization update failed: {e}")

    def _update_best_slam_metrics(self, epoch: int, slam_metrics: Dict[str, Any]):
        """æ›´æ–°æœ€ä½³SLAMæŒ‡æ ‡"""
        try:
            # æå–å…³é”®æŒ‡æ ‡
            ate = slam_metrics.get('slam_trajectory_metrics_ATE', float('inf'))
            map_score = slam_metrics.get('slam_detection_metrics_mAP', 0.0)

            # è®¡ç®—ç»¼åˆå¾—åˆ† (ATEè¶Šå°è¶Šå¥½ï¼ŒmAPè¶Šå¤§è¶Šå¥½)
            # å½’ä¸€åŒ–ï¼šATEä»¥0.5mä¸ºåŸºå‡†ï¼ŒmAPå·²åœ¨[0,1]èŒƒå›´
            normalized_ate = max(0, 1 - ate / 0.5)  # ATE=0æ—¶å¾—åˆ†1ï¼ŒATE>=0.5æ—¶å¾—åˆ†0
            combined_score = (normalized_ate + map_score) / 2

            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            if ate < self.best_slam_metrics['best_ate']:
                self.best_slam_metrics['best_ate'] = ate

            if map_score > self.best_slam_metrics['best_map']:
                self.best_slam_metrics['best_map'] = map_score

            if combined_score > self.best_slam_metrics['best_combined_score']:
                self.best_slam_metrics['best_combined_score'] = combined_score
                self.best_slam_metrics['best_epoch'] = epoch

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self._save_best_slam_model(epoch, slam_metrics)

        except Exception as e:
            warnings.warn(f"Failed to update best SLAM metrics: {e}")

    def _save_best_slam_model(self, epoch: int, slam_metrics: Dict[str, Any]):
        """ä¿å­˜æœ€ä½³SLAMæ¨¡å‹"""
        try:
            best_model_path = os.path.join(self.output_dir, "best_slam_model.pth")

            # ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'slam_metrics': slam_metrics,
                'best_slam_metrics': self.best_slam_metrics.copy()
            }

            torch.save(checkpoint, best_model_path)
            print(f"ğŸ’¾ Best SLAM model saved: {best_model_path}")

        except Exception as e:
            warnings.warn(f"Failed to save best SLAM model: {e}")

    def save_final_slam_report(self):
        """ä¿å­˜æœ€ç»ˆçš„SLAMè¯„ä¼°æŠ¥å‘Š"""
        try:
            report_data = {
                'training_completed': time.time(),
                'total_epochs': self.epoch,
                'best_slam_metrics': self.best_slam_metrics,
                'slam_metrics_history': self.slam_metrics_history[-10:],  # ä¿å­˜æœ€å10ä¸ªepoch
                'configuration': {
                    'slam_eval_enabled': self.enable_slam_eval,
                    'eval_frequency': self.eval_frequency,
                    'visualize_frequency': self.visualize_frequency
                }
            }

            # ä¿å­˜JSONæŠ¥å‘Š
            report_path = os.path.join(self.output_dir, "slam_evaluation_report.json")
            with open(report_path, 'w') as f:
                json.dump(report_data, f, indent=2)

            print(f"ğŸ“Š SLAM evaluation report saved: {report_path}")

            # åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š
            if self.slam_visualizer and self.slam_metrics_history:
                latest_metrics = self.slam_metrics_history[-1]['metrics']

                # é‡æ„æŒ‡æ ‡æ ¼å¼
                viz_report_metrics = {
                    'trajectory_metrics': {
                        key.replace('slam_trajectory_metrics_', ''): value
                        for key, value in latest_metrics.items()
                        if key.startswith('slam_trajectory_metrics_')
                    },
                    'detection_metrics': {
                        key.replace('slam_detection_metrics_', ''): value
                        for key, value in latest_metrics.items()
                        if key.startswith('slam_detection_metrics_')
                    },
                    'fusion_metrics': {
                        key.replace('slam_fusion_metrics_', ''): value
                        for key, value in latest_metrics.items()
                        if key.startswith('slam_fusion_metrics_')
                    },
                    'uncertainty_metrics': {
                        key.replace('slam_uncertainty_metrics_', ''): value
                        for key, value in latest_metrics.items()
                        if key.startswith('slam_uncertainty_metrics_')
                    }
                }

                viz_report_path = os.path.join(self.output_dir, "slam_evaluation_plots.png")
                self.slam_visualizer.create_evaluation_report_plots(
                    viz_report_metrics, viz_report_path
                )

            # è¾“å‡ºæœ€ç»ˆæ€»ç»“
            print(f"\nğŸ‰ SLAM Training Completed!")
            print(f"   Best ATE: {self.best_slam_metrics['best_ate']:.4f}m")
            print(f"   Best mAP: {self.best_slam_metrics['best_map']:.4f}")
            print(f"   Best Combined Score: {self.best_slam_metrics['best_combined_score']:.4f}")
            print(f"   Best Epoch: {self.best_slam_metrics['best_epoch']}")

        except Exception as e:
            warnings.warn(f"Failed to save final SLAM report: {e}")

    def train(self):
        """é‡å†™è®­ç»ƒå‡½æ•°ä»¥åŒ…å«SLAMè¯„ä¼°"""
        print(f"ğŸš€ Starting Enhanced SLAM Training")

        # è®¾ç½®SLAMè¯„ä¼°æ•°æ®é›†
        if self.enable_slam_eval:
            self.setup_slam_evaluation_datasets()

        # è°ƒç”¨çˆ¶ç±»è®­ç»ƒæ–¹æ³•
        result = super().train()

        # ä¿å­˜æœ€ç»ˆSLAMæŠ¥å‘Š
        self.save_final_slam_report()

        return result


def create_enhanced_slam_training_config(base_config: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ›å»ºå¢å¼ºSLAMè®­ç»ƒé…ç½®"""
    enhanced_config = base_config.copy()

    # æ·»åŠ SLAMè¯„ä¼°é…ç½®
    enhanced_config['slam_evaluation'] = {
        'enabled': True,
        'eval_frequency': 5,  # æ¯5ä¸ªepochè¯„ä¼°ä¸€æ¬¡
        'visualize_frequency': 10,  # æ¯10ä¸ªepochå¯è§†åŒ–ä¸€æ¬¡
        'save_visualizations': True,
        'evaluation_metrics': [
            'trajectory_accuracy',  # ATE, RPE
            'detection_performance',  # mAP
            'fusion_analysis',  # MoEä¸“å®¶åˆ©ç”¨ç‡
            'uncertainty_analysis'  # Kendallæƒé‡åˆ†æ
        ]
    }

    # è°ƒæ•´éªŒè¯é¢‘ç‡ä»¥é…åˆSLAMè¯„ä¼°
    if 'validation_frequency' in enhanced_config:
        enhanced_config['validation_frequency'] = min(
            enhanced_config['validation_frequency'],
            enhanced_config['slam_evaluation']['eval_frequency']
        )

    return enhanced_config


if __name__ == "__main__":
    # æµ‹è¯•å¢å¼ºçš„SLAMè®­ç»ƒå¾ªç¯
    print("ğŸ§ª Enhanced SLAM Training Loop - Test Mode")

    # åˆ›å»ºæµ‹è¯•é…ç½®
    test_config = {
        'batch_size': 2,
        'learning_rate': 1e-4,
        'max_epochs': 5,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'output_dir': 'outputs/test_enhanced_training',
        'slam_evaluation': {
            'enabled': True,
            'eval_frequency': 2,
            'visualize_frequency': 3,
            'save_visualizations': True
        }
    }

    try:
        # æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€è¦æä¾›çœŸå®çš„æ¨¡å‹å’Œæ•°æ®
        print("ğŸ”§ Enhanced SLAM training configuration created")
        print(f"   SLAM evaluation: {test_config['slam_evaluation']}")
        print("âœ… Configuration test passed!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()